{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS566_post_dac_dnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWVR5aj74Z6R",
        "outputId": "d51cbabf-b527-403e-c2ff-dae82a501024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# TODO check path\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.loss import _Loss\n",
        "import pdb\n",
        "import math\n",
        "import collections\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuQY2Mfc42oT",
        "outputId": "a1fdcd9c-887d-4409-fa49-bbe0abeda821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ref to: https://github.com/thulas/dac-label-noise/blob/master/dac_loss.py\n",
        "# import torch\n",
        "# from torch.autograd import Variable\n",
        "# import torch.nn.functional as F\n",
        "# from torch.nn.modules.loss import _Loss\n",
        "# import pdb\n",
        "# import math\n",
        "\n",
        "# for numerical stability\n",
        "epsilon = 1e-7\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG16_upd': [64, ('D', 0.3), 64, 'M', 128, ('D', 0.4), 128, 'M', 256, ('D', 0.4), 256, ('D', 0.4), 256, 'M', 512,\n",
        "                  ('D', 0.4), 512, ('D', 0.4), 512, 'M', 512, ('D', 0.4), 512, ('D', 0.4), 512, 'M', ('D', 0.5), ],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "# loss calculation and alpha-ramping are rolled into one function. This is invoked\n",
        "# after every iteration\n",
        "class dac_loss(_Loss):\n",
        "    def __init__(self, model, learn_epochs, total_epochs, use_cuda=False, cuda_device=None,\n",
        "                 alpha_final=1.0, alpha_init_factor=64.):\n",
        "        print(\"using dac loss function\\n\")\n",
        "        super(dac_loss, self).__init__()\n",
        "        self.model = model\n",
        "        self.learn_epochs = learn_epochs\n",
        "        self.total_epochs = total_epochs\n",
        "        self.alpha_final = alpha_final\n",
        "        self.alpha_init_factor = alpha_init_factor\n",
        "        self.use_cuda = use_cuda\n",
        "        self.cuda_device = cuda_device\n",
        "\n",
        "        self.alpha_var = None\n",
        "\n",
        "        self.alpha_thresh_ewma = None  # exponentially weighted moving average for alpha_thresh\n",
        "        self.alpha_thresh = None  # instantaneous alpha_thresh\n",
        "        self.ewma_mu = 0.05  # mu parameter for EWMA;\n",
        "        self.curr_alpha_factor = None  # for alpha initiliazation\n",
        "        self.alpha_inc = None  # linear increase factor of alpha during abstention phase\n",
        "        self.alpha_set_epoch = None\n",
        "\n",
        "    def __call__(self, input_batch, target_batch, epoch):\n",
        "        if epoch <= self.learn_epochs or not self.model.training:\n",
        "\n",
        "            loss = F.cross_entropy(input_batch, target_batch, reduction='none')\n",
        "            # return loss.mean()\n",
        "            if self.model.training:\n",
        "                h_c = F.cross_entropy(input_batch[:, 0:-1], target_batch, reduction='none')\n",
        "                p_out = torch.exp(F.log_softmax(input_batch, dim=1))\n",
        "                p_out_abstain = p_out[:, -1]\n",
        "\n",
        "                # update instantaneous alpha_thresh\n",
        "                self.alpha_thresh = Variable(((1. - p_out_abstain) * h_c).mean().data)\n",
        "                # update alpha_thresh_ewma\n",
        "                if self.alpha_thresh_ewma is None:\n",
        "                    self.alpha_thresh_ewma = self.alpha_thresh  # Variable(((1. - p_out_abstain)*h_c).mean().data)\n",
        "                else:\n",
        "                    self.alpha_thresh_ewma = Variable(self.ewma_mu * self.alpha_thresh.data + \\\n",
        "                                                      (1. - self.ewma_mu) * self.alpha_thresh_ewma.data)\n",
        "\n",
        "            return loss.mean()\n",
        "\n",
        "        else:\n",
        "            # calculate cross entropy only over true classes\n",
        "            h_c = F.cross_entropy(input_batch[:, 0:-1], target_batch, reduce=False)\n",
        "            p_out = torch.exp(F.log_softmax(input_batch, dim=1))\n",
        "            # probabilities of abstention  class\n",
        "            p_out_abstain = p_out[:, -1]\n",
        "\n",
        "            # avoid numerical instability by upper-bounding\n",
        "            # p_out_abstain to never be more than  1 - eps since we have to\n",
        "            # take log(1 - p_out_abstain) later.\n",
        "\n",
        "            if self.use_cuda:\n",
        "                p_out_abstain = torch.min(p_out_abstain,\n",
        "                                          Variable(torch.Tensor([1. - epsilon])).cuda(self.cuda_device))\n",
        "            else:\n",
        "                p_out_abstain = torch.min(p_out_abstain,\n",
        "                                          Variable(torch.Tensor([1. - epsilon])))\n",
        "\n",
        "            # update instantaneous alpha_thresh\n",
        "            self.alpha_thresh = Variable(((1. - p_out_abstain) * h_c).mean().data)\n",
        "\n",
        "            try:\n",
        "                # update alpha_thresh_ewma\n",
        "                if self.alpha_thresh_ewma is None:\n",
        "                    self.alpha_thresh_ewma = self.alpha_thresh  # Variable(((1. - p_out_abstain)*h_c).mean().data)\n",
        "                else:\n",
        "                    self.alpha_thresh_ewma = Variable(self.ewma_mu * self.alpha_thresh.data + \\\n",
        "                                                      (1. - self.ewma_mu) * self.alpha_thresh_ewma.data)\n",
        "\n",
        "                if self.alpha_var is None:  # hasn't been initialized. do it now\n",
        "                    # we create a freshVariable here so that the history of alpha_var\n",
        "                    # computation (which depends on alpha_thresh_ewma) is forgotten. This\n",
        "                    # makes self.alpha_var a leaf variable, which will not be differentiated.\n",
        "                    # aggressive initialization of alpha to jump start abstention\n",
        "                    self.alpha_var = Variable(self.alpha_thresh_ewma.data / self.alpha_init_factor)\n",
        "                    self.alpha_inc = (self.alpha_final - self.alpha_var.data) / (self.total_epochs - epoch)\n",
        "                    self.alpha_set_epoch = epoch\n",
        "\n",
        "                else:\n",
        "                    # we only update alpha every epoch\n",
        "                    if epoch > self.alpha_set_epoch:\n",
        "                        self.alpha_var = Variable(self.alpha_var.data + self.alpha_inc)\n",
        "                        self.alpha_set_epoch = epoch\n",
        "\n",
        "                loss = (1. - p_out_abstain) * h_c - \\\n",
        "                       self.alpha_var * torch.log(1. - p_out_abstain)\n",
        "\n",
        "                return loss.mean()\n",
        "            except RuntimeError as e:\n",
        "                # pdb.set_trace()\n",
        "                print(e)\n",
        "\n",
        "\n",
        "class cifar10_data_loader:\n",
        "\n",
        "    def __init__(self, batch_size, data_root='./gdrive/MyDrive/data', num_workers=2, num_classes=10, shuffle=True):\n",
        "        self.data_root = data_root\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.num_classes = num_classes\n",
        "        self.train_transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                                   transforms.RandomRotation(degrees=(-15, 15)),\n",
        "                                                   transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "                                                   transforms.ToTensor(),\n",
        "                                                   transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                                        (0.5, 0.5, 0.5))])\n",
        "        self.test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                                  transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                                       (0.5, 0.5, 0.5))])\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def img_show(self, img):\n",
        "        img = img / 2 + 0.5\n",
        "        np_img = img.numpy()\n",
        "        plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    def add_label_noise(self, trainset, noise_level):\n",
        "        num_train = len(trainset.targets)\n",
        "        num_noise = int(noise_level * num_train)\n",
        "        noise_indexes = random.sample(range(num_train), num_noise)\n",
        "        correct_target = np.copy(trainset.targets)\n",
        "        for idx in noise_indexes:\n",
        "            trainset.targets[idx] += random.randint(1, self.num_classes - 1)\n",
        "            trainset.targets[idx] %= self.num_classes\n",
        "\n",
        "        # validset.targets = np.copy(trainset.targets)\n",
        "        return correct_target, noise_indexes\n",
        "\n",
        "    def load(self, noise_level=0.1):\n",
        "        trainset = torchvision.datasets.CIFAR10(root=self.data_root, train=True,\n",
        "                                                download=True, transform=self.train_transform)\n",
        "        # validset = torchvision.datasets.CIFAR10(root=self.data_root, train=True,\n",
        "        #                                         download=True, transform=self.test_transform)\n",
        "        correct_target, noise_indexes = self.add_label_noise(trainset, noise_level)\n",
        "        trainloader = DataLoader(trainset, batch_size=self.batch_size,\n",
        "                                 shuffle=self.shuffle, num_workers=self.num_workers)\n",
        "\n",
        "        testset = torchvision.datasets.CIFAR10(root=self.data_root, train=False,\n",
        "                                               download=True, transform=self.test_transform)\n",
        "        testloader = DataLoader(testset, batch_size=self.batch_size,\n",
        "                                shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "        return trainloader, testloader, trainset, correct_target, noise_indexes\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vggname, use_DAC):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vggname])\n",
        "        final_classes = 11 if use_DAC else 10\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, final_classes),\n",
        "            # nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            elif type(x) == tuple:\n",
        "                rate = x[-1]\n",
        "                layers += [nn.Dropout(p=rate, inplace=True)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "                           nn.BatchNorm2d(x)]\n",
        "                in_channels = x\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def train(model, train_data, epoches, criterion, optimizer, scheduler, device, use_DAC):\n",
        "    for epoch in range(epoches):\n",
        "        running_loss = 0.0\n",
        "        if use_DAC:\n",
        "            for i, data in enumerate(train_data, 0):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels, epoch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "        else:\n",
        "            for i, data in enumerate(train_data, 0):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f'[epoch: {epoch}] loss: {running_loss / len(train_data):.5f}')\n",
        "\n",
        "\n",
        "def _train(trainloader):\n",
        "    # Load Data\n",
        "    # dataloader = cifar10_data_loader(batch_size)\n",
        "    # trainloader, testloader, trainset, correct_target, noise_indexes = dataloader.load(noise_level=noise_level)\n",
        "\n",
        "    model = VGG('VGG16_upd', use_DAC).to(device)\n",
        "    model = model.train()\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    if use_DAC:\n",
        "        criterion = dac_loss(model=model, learn_epochs=learn_epochs,\n",
        "                             total_epochs=epoches, use_cuda=use_cuda, cuda_device=device,\n",
        "                             alpha_final=alpha_final, alpha_init_factor=alpha_init_factor)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)\n",
        "\n",
        "    # Train model\n",
        "    train(model, trainloader, epoches, criterion, optimizer, scheduler, device, use_DAC)\n",
        "    print('Done!')\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_model(model, dir_name, model_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.mkdir(dir_name)\n",
        "    torch.save(model.state_dict(), dir_name + model_name + '.pth')\n",
        "\n",
        "\n",
        "def load_model(model, dir_name, model_name):\n",
        "    # model = VGG('VGG16_upd', use_DAC).to(device)\n",
        "    model.load_state_dict(torch.load(dir_name + model_name))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def filter_train_set(trainloader, testloader, trainset, correct_target, noise_indexes):\n",
        "    kept_data_idx, abstain_data_idx = [], []\n",
        "    with torch.no_grad():\n",
        "        i = 0\n",
        "        for data in trainloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred_idx = predicted == labels\n",
        "            for j in range(len(pred_idx)):\n",
        "                if pred_idx[j]:\n",
        "                    kept_data_idx.append(i*batch_size + j)\n",
        "                else:\n",
        "                    abstain_data_idx.append(i*batch_size + j)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "    mask = np.ones(len(trainset.data), dtype=bool)\n",
        "    mask[abstain_data_idx] = False\n",
        "    trainset.data = trainset.data[mask]\n",
        "    trainset.targets = np.array([trainset.targets[i] for i in kept_data_idx])\n",
        "\n",
        "    trainloader_new = DataLoader(trainset, batch_size=batch_size,\n",
        "                                 shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    print(\"number of abstained set: {}, number of kept data set: {}\".format(len(abstain_data_idx), len(kept_data_idx)))\n",
        "    if len(noise_indexes):\n",
        "        print(\"percentage noise filtered: {}\".format(len(set(noise_indexes) & set(abstain_data_idx)) / len(noise_indexes)))\n",
        "    print(\"percentage noise remained: {}\".format(len(set(noise_indexes) & set(kept_data_idx)) / len(kept_data_idx)))\n",
        "\n",
        "    return trainloader_new\n",
        "\n",
        "\n",
        "def post_dac_dnn(trainloader_new):\n",
        "    # Train a new model to make prediction\n",
        "    model_new = VGG('VGG16_upd', False).to(device)\n",
        "    model_new = model_new.train()\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion_new = nn.CrossEntropyLoss()\n",
        "    optimizer_new = optim.SGD(model_new.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "    scheduler_new = optim.lr_scheduler.StepLR(optimizer_new, step_size=25, gamma=0.5)\n",
        "\n",
        "    # Train model\n",
        "    train(model_new, trainloader_new, epoches, criterion_new, optimizer_new, scheduler_new, device, False)\n",
        "    return model_new\n",
        "\n",
        "def evaluate(model_new, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # kappa = []\n",
        "    # residuals = []\n",
        "    model_new.eval()\n",
        "\n",
        "    nb_classes = 10\n",
        "\n",
        "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "    with torch.no_grad():\n",
        "        for (inputs, classes) in testloader:\n",
        "            inputs = inputs.to(device)\n",
        "            classes = classes.to(device)\n",
        "            outputs = model_new(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += classes.size(0)\n",
        "            correct += (preds == classes).sum().item()\n",
        "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "    \n",
        "    print(torch.diagonal(confusion_matrix, 0) / 1000)\n",
        "\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     for data in testloader:\n",
        "    #         images, labels = data[0].to(device), data[1].to(device)\n",
        "            # calculate outputs by running images through the network\n",
        "            # outputs = model_new(images)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            # _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # total += labels.size(0)\n",
        "            # correct += (predicted == labels).sum().item()\n",
        "            # res = predicted != labels\n",
        "            # kap = torch.max(nn.Softmax()(outputs), 1)[0]\n",
        "            # kappa.append(kap)\n",
        "            # residuals.append(res)\n",
        "\n",
        "    # kappa = torch.cat(kappa)\n",
        "    # residuals = torch.cat(residuals)\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
        "    return None, None\n",
        "    # return kappa, residuals\n",
        "\n",
        "def plot_cr(kappa, residuals, path):\n",
        "    risk = []\n",
        "    cov = []\n",
        "    probs = kappa.cpu()\n",
        "    TNF = residuals.cpu()\n",
        "    for theta in range(0, 1000):\n",
        "        theta = theta / 1000\n",
        "\n",
        "        if len(TNF[probs >= theta]) == 0:\n",
        "            continue\n",
        "        risk.append(sum(TNF[probs >= theta]) / len(TNF[probs >= theta]))\n",
        "        cov.append(len(TNF[probs >= theta]) / len(TNF))\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(cov, risk)\n",
        "    # plt.savefig(path)"
      ],
      "metadata": {
        "id": "5qEfokgK5GtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# params\n",
        "batch_size = 128\n",
        "epoches = 250\n",
        "learning_rate = 0.1\n",
        "lr_decay = 1e-6\n",
        "weight_decay = 0.0005\n",
        "noise_level = 0.4\n",
        "num_workers = 2\n",
        "use_DAC = True\n",
        "learn_epochs = 10\n",
        "use_cuda = True\n",
        "alpha_final = 1.0\n",
        "alpha_init_factor = 64.0\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "dir_name = '/content/drive/MyDrive/CS566/post_dac_dnn/'"
      ],
      "metadata": {
        "id": "pv6Arv2n5NEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for noise_level in [0, 0.1, 0.2, 0.4]: \n",
        "    print(\"Training dac and post_dac at noise level {}\".format(noise_level))\n",
        "    dataloader = cifar10_data_loader(batch_size, shuffle=False)\n",
        "    trainloader, testloader, trainset, correct_target, noise_indexes = dataloader.load(noise_level=noise_level)\n",
        "    model = _train(trainloader)\n",
        "    model_name = 'vgg16_dac_nr_{}_epoch_{}'.format(noise_level, epoches)\n",
        "    save_model(model, dir_name, model_name)\n",
        "    trainloader_new = filter_train_set(trainloader, testloader, trainset, correct_target, noise_indexes)\n",
        "    new_model = post_dac_dnn(trainloader_new)\n",
        "    new_model_name = 'vgg16_post_dac_dnn_nr_{}_epoch_{}'.format(noise_level, epoches)\n",
        "    save_model(new_model, dir_name, new_model_name)\n",
        "    kappa, residuals = evaluate(new_model, testloader)\n",
        "    plot_cr(kappa, residuals, dir_name + 'cr_vgg16_post_dac_dnn_nr_{}_epoch_{}.png'.format(noise_level, epoches))\n",
        "    print()\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v38LQJtL5SrP",
        "outputId": "56fa93dd-947f-4e40-9546-68c7bd13ff53"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dac and post_dac at noise level 0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "using dac loss function\n",
            "\n",
            "[epoch: 0] loss: 2.58542\n",
            "[epoch: 1] loss: 2.03960\n",
            "[epoch: 2] loss: 1.81670\n",
            "[epoch: 3] loss: 1.68036\n",
            "[epoch: 4] loss: 1.54295\n",
            "[epoch: 5] loss: 1.39484\n",
            "[epoch: 6] loss: 1.26664\n",
            "[epoch: 7] loss: 1.16613\n",
            "[epoch: 8] loss: 1.08573\n",
            "[epoch: 9] loss: 1.01744\n",
            "[epoch: 10] loss: 0.98271\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch: 11] loss: 0.54285\n",
            "[epoch: 12] loss: 0.08848\n",
            "[epoch: 13] loss: 0.10135\n",
            "[epoch: 14] loss: 0.11793\n",
            "[epoch: 15] loss: 0.13536\n",
            "[epoch: 16] loss: 0.15481\n",
            "[epoch: 17] loss: 0.16846\n",
            "[epoch: 18] loss: 0.18336\n",
            "[epoch: 19] loss: 0.20262\n",
            "[epoch: 20] loss: 0.21063\n",
            "[epoch: 21] loss: 0.22552\n",
            "[epoch: 22] loss: 0.23746\n",
            "[epoch: 23] loss: 0.25021\n",
            "[epoch: 24] loss: 0.26465\n",
            "[epoch: 25] loss: 0.25901\n",
            "[epoch: 26] loss: 0.26787\n",
            "[epoch: 27] loss: 0.27476\n",
            "[epoch: 28] loss: 0.28467\n",
            "[epoch: 29] loss: 0.29064\n",
            "[epoch: 30] loss: 0.30411\n",
            "[epoch: 31] loss: 0.30553\n",
            "[epoch: 32] loss: 0.31160\n",
            "[epoch: 33] loss: 0.31881\n",
            "[epoch: 34] loss: 0.32749\n",
            "[epoch: 35] loss: 0.33794\n",
            "[epoch: 36] loss: 0.33632\n",
            "[epoch: 37] loss: 0.34218\n",
            "[epoch: 38] loss: 0.34875\n",
            "[epoch: 39] loss: 0.35477\n",
            "[epoch: 40] loss: 0.36188\n",
            "[epoch: 41] loss: 0.36700\n",
            "[epoch: 42] loss: 0.36794\n",
            "[epoch: 43] loss: 0.37538\n",
            "[epoch: 44] loss: 0.37821\n",
            "[epoch: 45] loss: 0.37930\n",
            "[epoch: 46] loss: 0.38558\n",
            "[epoch: 47] loss: 0.39087\n",
            "[epoch: 48] loss: 0.39032\n",
            "[epoch: 49] loss: 0.39805\n",
            "[epoch: 50] loss: 0.36296\n",
            "[epoch: 51] loss: 0.35745\n",
            "[epoch: 52] loss: 0.36365\n",
            "[epoch: 53] loss: 0.36465\n",
            "[epoch: 54] loss: 0.36795\n",
            "[epoch: 55] loss: 0.36924\n",
            "[epoch: 56] loss: 0.37290\n",
            "[epoch: 57] loss: 0.37878\n",
            "[epoch: 58] loss: 0.37563\n",
            "[epoch: 59] loss: 0.37736\n",
            "[epoch: 60] loss: 0.37935\n",
            "[epoch: 61] loss: 0.37872\n",
            "[epoch: 62] loss: 0.38708\n",
            "[epoch: 63] loss: 0.38467\n",
            "[epoch: 64] loss: 0.38495\n",
            "[epoch: 65] loss: 0.39050\n",
            "[epoch: 66] loss: 0.38715\n",
            "[epoch: 67] loss: 0.39475\n",
            "[epoch: 68] loss: 0.39371\n",
            "[epoch: 69] loss: 0.39315\n",
            "[epoch: 70] loss: 0.39248\n",
            "[epoch: 71] loss: 0.39161\n",
            "[epoch: 72] loss: 0.39664\n",
            "[epoch: 73] loss: 0.39875\n",
            "[epoch: 74] loss: 0.39464\n",
            "[epoch: 75] loss: 0.35509\n",
            "[epoch: 76] loss: 0.34404\n",
            "[epoch: 77] loss: 0.33966\n",
            "[epoch: 78] loss: 0.34250\n",
            "[epoch: 79] loss: 0.34022\n",
            "[epoch: 80] loss: 0.34081\n",
            "[epoch: 81] loss: 0.33897\n",
            "[epoch: 82] loss: 0.34778\n",
            "[epoch: 83] loss: 0.34356\n",
            "[epoch: 84] loss: 0.34124\n",
            "[epoch: 85] loss: 0.34447\n",
            "[epoch: 86] loss: 0.34493\n",
            "[epoch: 87] loss: 0.34837\n",
            "[epoch: 88] loss: 0.34597\n",
            "[epoch: 89] loss: 0.34648\n",
            "[epoch: 90] loss: 0.34692\n",
            "[epoch: 91] loss: 0.34623\n",
            "[epoch: 92] loss: 0.34800\n",
            "[epoch: 93] loss: 0.34454\n",
            "[epoch: 94] loss: 0.34381\n",
            "[epoch: 95] loss: 0.34972\n",
            "[epoch: 96] loss: 0.34581\n",
            "[epoch: 97] loss: 0.34753\n",
            "[epoch: 98] loss: 0.35458\n",
            "[epoch: 99] loss: 0.34540\n",
            "[epoch: 100] loss: 0.30804\n",
            "[epoch: 101] loss: 0.29539\n",
            "[epoch: 102] loss: 0.29299\n",
            "[epoch: 103] loss: 0.28656\n",
            "[epoch: 104] loss: 0.28795\n",
            "[epoch: 105] loss: 0.28775\n",
            "[epoch: 106] loss: 0.28672\n",
            "[epoch: 107] loss: 0.28606\n",
            "[epoch: 108] loss: 0.28259\n",
            "[epoch: 109] loss: 0.28577\n",
            "[epoch: 110] loss: 0.28376\n",
            "[epoch: 111] loss: 0.28384\n",
            "[epoch: 112] loss: 0.28900\n",
            "[epoch: 113] loss: 0.28340\n",
            "[epoch: 114] loss: 0.28291\n",
            "[epoch: 115] loss: 0.28324\n",
            "[epoch: 116] loss: 0.28365\n",
            "[epoch: 117] loss: 0.28378\n",
            "[epoch: 118] loss: 0.27911\n",
            "[epoch: 119] loss: 0.27988\n",
            "[epoch: 120] loss: 0.28252\n",
            "[epoch: 121] loss: 0.28599\n",
            "[epoch: 122] loss: 0.28214\n",
            "[epoch: 123] loss: 0.28020\n",
            "[epoch: 124] loss: 0.28185\n",
            "[epoch: 125] loss: 0.25255\n",
            "[epoch: 126] loss: 0.23965\n",
            "[epoch: 127] loss: 0.23576\n",
            "[epoch: 128] loss: 0.23519\n",
            "[epoch: 129] loss: 0.23418\n",
            "[epoch: 130] loss: 0.22716\n",
            "[epoch: 131] loss: 0.22717\n",
            "[epoch: 132] loss: 0.22258\n",
            "[epoch: 133] loss: 0.22614\n",
            "[epoch: 134] loss: 0.22590\n",
            "[epoch: 135] loss: 0.22367\n",
            "[epoch: 136] loss: 0.22209\n",
            "[epoch: 137] loss: 0.22538\n",
            "[epoch: 138] loss: 0.21817\n",
            "[epoch: 139] loss: 0.21800\n",
            "[epoch: 140] loss: 0.22200\n",
            "[epoch: 141] loss: 0.21702\n",
            "[epoch: 142] loss: 0.21587\n",
            "[epoch: 143] loss: 0.21326\n",
            "[epoch: 144] loss: 0.21413\n",
            "[epoch: 145] loss: 0.22100\n",
            "[epoch: 146] loss: 0.21363\n",
            "[epoch: 147] loss: 0.21667\n",
            "[epoch: 148] loss: 0.21149\n",
            "[epoch: 149] loss: 0.21536\n",
            "[epoch: 150] loss: 0.19755\n",
            "[epoch: 151] loss: 0.18614\n",
            "[epoch: 152] loss: 0.18416\n",
            "[epoch: 153] loss: 0.17973\n",
            "[epoch: 154] loss: 0.18006\n",
            "[epoch: 155] loss: 0.17492\n",
            "[epoch: 156] loss: 0.17526\n",
            "[epoch: 157] loss: 0.17827\n",
            "[epoch: 158] loss: 0.17530\n",
            "[epoch: 159] loss: 0.17351\n",
            "[epoch: 160] loss: 0.17108\n",
            "[epoch: 161] loss: 0.17012\n",
            "[epoch: 162] loss: 0.16771\n",
            "[epoch: 163] loss: 0.16706\n",
            "[epoch: 164] loss: 0.16763\n",
            "[epoch: 165] loss: 0.16147\n",
            "[epoch: 166] loss: 0.16425\n",
            "[epoch: 167] loss: 0.16441\n",
            "[epoch: 168] loss: 0.16541\n",
            "[epoch: 169] loss: 0.16393\n",
            "[epoch: 170] loss: 0.16527\n",
            "[epoch: 171] loss: 0.16302\n",
            "[epoch: 172] loss: 0.16474\n",
            "[epoch: 173] loss: 0.16145\n",
            "[epoch: 174] loss: 0.15536\n",
            "[epoch: 175] loss: 0.14739\n",
            "[epoch: 176] loss: 0.14345\n",
            "[epoch: 177] loss: 0.14192\n",
            "[epoch: 178] loss: 0.13968\n",
            "[epoch: 179] loss: 0.13796\n",
            "[epoch: 180] loss: 0.13730\n",
            "[epoch: 181] loss: 0.13427\n",
            "[epoch: 182] loss: 0.13245\n",
            "[epoch: 183] loss: 0.13443\n",
            "[epoch: 184] loss: 0.13086\n",
            "[epoch: 185] loss: 0.12966\n",
            "[epoch: 186] loss: 0.13296\n",
            "[epoch: 187] loss: 0.12881\n",
            "[epoch: 188] loss: 0.13028\n",
            "[epoch: 189] loss: 0.12995\n",
            "[epoch: 190] loss: 0.12846\n",
            "[epoch: 191] loss: 0.12862\n",
            "[epoch: 192] loss: 0.12604\n",
            "[epoch: 193] loss: 0.12656\n",
            "[epoch: 194] loss: 0.12425\n",
            "[epoch: 195] loss: 0.12445\n",
            "[epoch: 196] loss: 0.12169\n",
            "[epoch: 197] loss: 0.12603\n",
            "[epoch: 198] loss: 0.12620\n",
            "[epoch: 199] loss: 0.12183\n",
            "[epoch: 200] loss: 0.11937\n",
            "[epoch: 201] loss: 0.11443\n",
            "[epoch: 202] loss: 0.11420\n",
            "[epoch: 203] loss: 0.11052\n",
            "[epoch: 204] loss: 0.11441\n",
            "[epoch: 205] loss: 0.11167\n",
            "[epoch: 206] loss: 0.11188\n",
            "[epoch: 207] loss: 0.11434\n",
            "[epoch: 208] loss: 0.10908\n",
            "[epoch: 209] loss: 0.10778\n",
            "[epoch: 210] loss: 0.10667\n",
            "[epoch: 211] loss: 0.10734\n",
            "[epoch: 212] loss: 0.10284\n",
            "[epoch: 213] loss: 0.10613\n",
            "[epoch: 214] loss: 0.10198\n",
            "[epoch: 215] loss: 0.10222\n",
            "[epoch: 216] loss: 0.10512\n",
            "[epoch: 217] loss: 0.10310\n",
            "[epoch: 218] loss: 0.10367\n",
            "[epoch: 219] loss: 0.10384\n",
            "[epoch: 220] loss: 0.10463\n",
            "[epoch: 221] loss: 0.10492\n",
            "[epoch: 222] loss: 0.10020\n",
            "[epoch: 223] loss: 0.10119\n",
            "[epoch: 224] loss: 0.09810\n",
            "[epoch: 225] loss: 0.09475\n",
            "[epoch: 226] loss: 0.09916\n",
            "[epoch: 227] loss: 0.09557\n",
            "[epoch: 228] loss: 0.09563\n",
            "[epoch: 229] loss: 0.09355\n",
            "[epoch: 230] loss: 0.09268\n",
            "[epoch: 231] loss: 0.09558\n",
            "[epoch: 232] loss: 0.09694\n",
            "[epoch: 233] loss: 0.09208\n",
            "[epoch: 234] loss: 0.09334\n",
            "[epoch: 235] loss: 0.09465\n",
            "[epoch: 236] loss: 0.09298\n",
            "[epoch: 237] loss: 0.09307\n",
            "[epoch: 238] loss: 0.09422\n",
            "[epoch: 239] loss: 0.09244\n",
            "[epoch: 240] loss: 0.09196\n",
            "[epoch: 241] loss: 0.09368\n",
            "[epoch: 242] loss: 0.08923\n",
            "[epoch: 243] loss: 0.08822\n",
            "[epoch: 244] loss: 0.08858\n",
            "[epoch: 245] loss: 0.09079\n",
            "[epoch: 246] loss: 0.09233\n",
            "[epoch: 247] loss: 0.08756\n",
            "[epoch: 248] loss: 0.08832\n",
            "[epoch: 249] loss: 0.09017\n",
            "Done!\n",
            "number of abstained set: 1499, number of kept data set: 48501\n",
            "percentage noise remained: 0.0\n",
            "[epoch: 0] loss: 2.57780\n",
            "[epoch: 1] loss: 1.81695\n",
            "[epoch: 2] loss: 1.59973\n",
            "[epoch: 3] loss: 1.44016\n",
            "[epoch: 4] loss: 1.27946\n",
            "[epoch: 5] loss: 1.17396\n",
            "[epoch: 6] loss: 1.07953\n",
            "[epoch: 7] loss: 1.00312\n",
            "[epoch: 8] loss: 0.96732\n",
            "[epoch: 9] loss: 0.91715\n",
            "[epoch: 10] loss: 0.89090\n",
            "[epoch: 11] loss: 0.86978\n",
            "[epoch: 12] loss: 0.85052\n",
            "[epoch: 13] loss: 0.83069\n",
            "[epoch: 14] loss: 0.81476\n",
            "[epoch: 15] loss: 0.79872\n",
            "[epoch: 16] loss: 0.77912\n",
            "[epoch: 17] loss: 0.78321\n",
            "[epoch: 18] loss: 0.76030\n",
            "[epoch: 19] loss: 0.75260\n",
            "[epoch: 20] loss: 0.74200\n",
            "[epoch: 21] loss: 0.73453\n",
            "[epoch: 22] loss: 0.73048\n",
            "[epoch: 23] loss: 0.72533\n",
            "[epoch: 24] loss: 0.72326\n",
            "[epoch: 25] loss: 0.58719\n",
            "[epoch: 26] loss: 0.56061\n",
            "[epoch: 27] loss: 0.56083\n",
            "[epoch: 28] loss: 0.55723\n",
            "[epoch: 29] loss: 0.55771\n",
            "[epoch: 30] loss: 0.54866\n",
            "[epoch: 31] loss: 0.54876\n",
            "[epoch: 32] loss: 0.54871\n",
            "[epoch: 33] loss: 0.54127\n",
            "[epoch: 34] loss: 0.54930\n",
            "[epoch: 35] loss: 0.54322\n",
            "[epoch: 36] loss: 0.53588\n",
            "[epoch: 37] loss: 0.52863\n",
            "[epoch: 38] loss: 0.53111\n",
            "[epoch: 39] loss: 0.52457\n",
            "[epoch: 40] loss: 0.52171\n",
            "[epoch: 41] loss: 0.51675\n",
            "[epoch: 42] loss: 0.51798\n",
            "[epoch: 43] loss: 0.51335\n",
            "[epoch: 44] loss: 0.51534\n",
            "[epoch: 45] loss: 0.51617\n",
            "[epoch: 46] loss: 0.51469\n",
            "[epoch: 47] loss: 0.50629\n",
            "[epoch: 48] loss: 0.50473\n",
            "[epoch: 49] loss: 0.50581\n",
            "[epoch: 50] loss: 0.40093\n",
            "[epoch: 51] loss: 0.38579\n",
            "[epoch: 52] loss: 0.37443\n",
            "[epoch: 53] loss: 0.37650\n",
            "[epoch: 54] loss: 0.36752\n",
            "[epoch: 55] loss: 0.37327\n",
            "[epoch: 56] loss: 0.37362\n",
            "[epoch: 57] loss: 0.36753\n",
            "[epoch: 58] loss: 0.37055\n",
            "[epoch: 59] loss: 0.37469\n",
            "[epoch: 60] loss: 0.37415\n",
            "[epoch: 61] loss: 0.37282\n",
            "[epoch: 62] loss: 0.37047\n",
            "[epoch: 63] loss: 0.36779\n",
            "[epoch: 64] loss: 0.37145\n",
            "[epoch: 65] loss: 0.36996\n",
            "[epoch: 66] loss: 0.36870\n",
            "[epoch: 67] loss: 0.37095\n",
            "[epoch: 68] loss: 0.36632\n",
            "[epoch: 69] loss: 0.36356\n",
            "[epoch: 70] loss: 0.36793\n",
            "[epoch: 71] loss: 0.36752\n",
            "[epoch: 72] loss: 0.36317\n",
            "[epoch: 73] loss: 0.36361\n",
            "[epoch: 74] loss: 0.36315\n",
            "[epoch: 75] loss: 0.29459\n",
            "[epoch: 76] loss: 0.26750\n",
            "[epoch: 77] loss: 0.26657\n",
            "[epoch: 78] loss: 0.25789\n",
            "[epoch: 79] loss: 0.25648\n",
            "[epoch: 80] loss: 0.25732\n",
            "[epoch: 81] loss: 0.25405\n",
            "[epoch: 82] loss: 0.25295\n",
            "[epoch: 83] loss: 0.25722\n",
            "[epoch: 84] loss: 0.25485\n",
            "[epoch: 85] loss: 0.25539\n",
            "[epoch: 86] loss: 0.25176\n",
            "[epoch: 87] loss: 0.25363\n",
            "[epoch: 88] loss: 0.25831\n",
            "[epoch: 89] loss: 0.25529\n",
            "[epoch: 90] loss: 0.25540\n",
            "[epoch: 91] loss: 0.25078\n",
            "[epoch: 92] loss: 0.25366\n",
            "[epoch: 93] loss: 0.25259\n",
            "[epoch: 94] loss: 0.25441\n",
            "[epoch: 95] loss: 0.25052\n",
            "[epoch: 96] loss: 0.25742\n",
            "[epoch: 97] loss: 0.24910\n",
            "[epoch: 98] loss: 0.25384\n",
            "[epoch: 99] loss: 0.25382\n",
            "[epoch: 100] loss: 0.20475\n",
            "[epoch: 101] loss: 0.18907\n",
            "[epoch: 102] loss: 0.18075\n",
            "[epoch: 103] loss: 0.18339\n",
            "[epoch: 104] loss: 0.17726\n",
            "[epoch: 105] loss: 0.17459\n",
            "[epoch: 106] loss: 0.17291\n",
            "[epoch: 107] loss: 0.16834\n",
            "[epoch: 108] loss: 0.17204\n",
            "[epoch: 109] loss: 0.16832\n",
            "[epoch: 110] loss: 0.16865\n",
            "[epoch: 111] loss: 0.16983\n",
            "[epoch: 112] loss: 0.16534\n",
            "[epoch: 113] loss: 0.16998\n",
            "[epoch: 114] loss: 0.16819\n",
            "[epoch: 115] loss: 0.16888\n",
            "[epoch: 116] loss: 0.16510\n",
            "[epoch: 117] loss: 0.16943\n",
            "[epoch: 118] loss: 0.16348\n",
            "[epoch: 119] loss: 0.17016\n",
            "[epoch: 120] loss: 0.16696\n",
            "[epoch: 121] loss: 0.16941\n",
            "[epoch: 122] loss: 0.16631\n",
            "[epoch: 123] loss: 0.16861\n",
            "[epoch: 124] loss: 0.16898\n",
            "[epoch: 125] loss: 0.13794\n",
            "[epoch: 126] loss: 0.12947\n",
            "[epoch: 127] loss: 0.12229\n",
            "[epoch: 128] loss: 0.11934\n",
            "[epoch: 129] loss: 0.11986\n",
            "[epoch: 130] loss: 0.12024\n",
            "[epoch: 131] loss: 0.11492\n",
            "[epoch: 132] loss: 0.11665\n",
            "[epoch: 133] loss: 0.11335\n",
            "[epoch: 134] loss: 0.11475\n",
            "[epoch: 135] loss: 0.10672\n",
            "[epoch: 136] loss: 0.11114\n",
            "[epoch: 137] loss: 0.10948\n",
            "[epoch: 138] loss: 0.11115\n",
            "[epoch: 139] loss: 0.11077\n",
            "[epoch: 140] loss: 0.10759\n",
            "[epoch: 141] loss: 0.11019\n",
            "[epoch: 142] loss: 0.11295\n",
            "[epoch: 143] loss: 0.11048\n",
            "[epoch: 144] loss: 0.10593\n",
            "[epoch: 145] loss: 0.10754\n",
            "[epoch: 146] loss: 0.10517\n",
            "[epoch: 147] loss: 0.10582\n",
            "[epoch: 148] loss: 0.10988\n",
            "[epoch: 149] loss: 0.10686\n",
            "[epoch: 150] loss: 0.09517\n",
            "[epoch: 151] loss: 0.09029\n",
            "[epoch: 152] loss: 0.08119\n",
            "[epoch: 153] loss: 0.08138\n",
            "[epoch: 154] loss: 0.07948\n",
            "[epoch: 155] loss: 0.07935\n",
            "[epoch: 156] loss: 0.08016\n",
            "[epoch: 157] loss: 0.07864\n",
            "[epoch: 158] loss: 0.07799\n",
            "[epoch: 159] loss: 0.08003\n",
            "[epoch: 160] loss: 0.07506\n",
            "[epoch: 161] loss: 0.07752\n",
            "[epoch: 162] loss: 0.07579\n",
            "[epoch: 163] loss: 0.07174\n",
            "[epoch: 164] loss: 0.07658\n",
            "[epoch: 165] loss: 0.07161\n",
            "[epoch: 166] loss: 0.07435\n",
            "[epoch: 167] loss: 0.07091\n",
            "[epoch: 168] loss: 0.07038\n",
            "[epoch: 169] loss: 0.07102\n",
            "[epoch: 170] loss: 0.07070\n",
            "[epoch: 171] loss: 0.07057\n",
            "[epoch: 172] loss: 0.07539\n",
            "[epoch: 173] loss: 0.07102\n",
            "[epoch: 174] loss: 0.07059\n",
            "[epoch: 175] loss: 0.06378\n",
            "[epoch: 176] loss: 0.06225\n",
            "[epoch: 177] loss: 0.05986\n",
            "[epoch: 178] loss: 0.05785\n",
            "[epoch: 179] loss: 0.05945\n",
            "[epoch: 180] loss: 0.05745\n",
            "[epoch: 181] loss: 0.05866\n",
            "[epoch: 182] loss: 0.05903\n",
            "[epoch: 183] loss: 0.05646\n",
            "[epoch: 184] loss: 0.05548\n",
            "[epoch: 185] loss: 0.05465\n",
            "[epoch: 186] loss: 0.05229\n",
            "[epoch: 187] loss: 0.05554\n",
            "[epoch: 188] loss: 0.05348\n",
            "[epoch: 189] loss: 0.05540\n",
            "[epoch: 190] loss: 0.05788\n",
            "[epoch: 191] loss: 0.05502\n",
            "[epoch: 192] loss: 0.05454\n",
            "[epoch: 193] loss: 0.05466\n",
            "[epoch: 194] loss: 0.05502\n",
            "[epoch: 195] loss: 0.05317\n",
            "[epoch: 196] loss: 0.05489\n",
            "[epoch: 197] loss: 0.05353\n",
            "[epoch: 198] loss: 0.05152\n",
            "[epoch: 199] loss: 0.05248\n",
            "[epoch: 200] loss: 0.05260\n",
            "[epoch: 201] loss: 0.04765\n",
            "[epoch: 202] loss: 0.04783\n",
            "[epoch: 203] loss: 0.04676\n",
            "[epoch: 204] loss: 0.04784\n",
            "[epoch: 205] loss: 0.04674\n",
            "[epoch: 206] loss: 0.04609\n",
            "[epoch: 207] loss: 0.04564\n",
            "[epoch: 208] loss: 0.04578\n",
            "[epoch: 209] loss: 0.04627\n",
            "[epoch: 210] loss: 0.04597\n",
            "[epoch: 211] loss: 0.04517\n",
            "[epoch: 212] loss: 0.04110\n",
            "[epoch: 213] loss: 0.04394\n",
            "[epoch: 214] loss: 0.04372\n",
            "[epoch: 215] loss: 0.04541\n",
            "[epoch: 216] loss: 0.04478\n",
            "[epoch: 217] loss: 0.04374\n",
            "[epoch: 218] loss: 0.04409\n",
            "[epoch: 219] loss: 0.04199\n",
            "[epoch: 220] loss: 0.04425\n",
            "[epoch: 221] loss: 0.04441\n",
            "[epoch: 222] loss: 0.04457\n",
            "[epoch: 223] loss: 0.04365\n",
            "[epoch: 224] loss: 0.04147\n",
            "[epoch: 225] loss: 0.04287\n",
            "[epoch: 226] loss: 0.04048\n",
            "[epoch: 227] loss: 0.04125\n",
            "[epoch: 228] loss: 0.04079\n",
            "[epoch: 229] loss: 0.03921\n",
            "[epoch: 230] loss: 0.04019\n",
            "[epoch: 231] loss: 0.04029\n",
            "[epoch: 232] loss: 0.04348\n",
            "[epoch: 233] loss: 0.03851\n",
            "[epoch: 234] loss: 0.04004\n",
            "[epoch: 235] loss: 0.03990\n",
            "[epoch: 236] loss: 0.04097\n",
            "[epoch: 237] loss: 0.03948\n",
            "[epoch: 238] loss: 0.03992\n",
            "[epoch: 239] loss: 0.04097\n",
            "[epoch: 240] loss: 0.04012\n",
            "[epoch: 241] loss: 0.04100\n",
            "[epoch: 242] loss: 0.03967\n",
            "[epoch: 243] loss: 0.03850\n",
            "[epoch: 244] loss: 0.03924\n",
            "[epoch: 245] loss: 0.03978\n",
            "[epoch: 246] loss: 0.03888\n",
            "[epoch: 247] loss: 0.03926\n",
            "[epoch: 248] loss: 0.03737\n",
            "[epoch: 249] loss: 0.03854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:333: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 89.59 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Training dac and post_dac at noise level 0.1\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "using dac loss function\n",
            "\n",
            "[epoch: 0] loss: 2.71196\n",
            "[epoch: 1] loss: 2.09844\n",
            "[epoch: 2] loss: 1.96305\n",
            "[epoch: 3] loss: 1.87415\n",
            "[epoch: 4] loss: 1.78659\n",
            "[epoch: 5] loss: 1.77698\n",
            "[epoch: 6] loss: 1.63672\n",
            "[epoch: 7] loss: 1.56294\n",
            "[epoch: 8] loss: 1.50707\n",
            "[epoch: 9] loss: 1.44881\n",
            "[epoch: 10] loss: 1.39827\n",
            "[epoch: 11] loss: 0.60384\n",
            "[epoch: 12] loss: 0.12584\n",
            "[epoch: 13] loss: 0.13930\n",
            "[epoch: 14] loss: 0.15306\n",
            "[epoch: 15] loss: 0.16687\n",
            "[epoch: 16] loss: 0.18121\n",
            "[epoch: 17] loss: 0.19564\n",
            "[epoch: 18] loss: 0.21068\n",
            "[epoch: 19] loss: 0.22406\n",
            "[epoch: 20] loss: 0.23862\n",
            "[epoch: 21] loss: 0.25176\n",
            "[epoch: 22] loss: 0.26308\n",
            "[epoch: 23] loss: 0.27513\n",
            "[epoch: 24] loss: 0.28658\n",
            "[epoch: 25] loss: 0.28962\n",
            "[epoch: 26] loss: 0.30012\n",
            "[epoch: 27] loss: 0.31066\n",
            "[epoch: 28] loss: 0.32099\n",
            "[epoch: 29] loss: 0.33080\n",
            "[epoch: 30] loss: 0.33975\n",
            "[epoch: 31] loss: 0.34919\n",
            "[epoch: 32] loss: 0.35976\n",
            "[epoch: 33] loss: 0.36880\n",
            "[epoch: 34] loss: 0.37775\n",
            "[epoch: 35] loss: 0.38625\n",
            "[epoch: 36] loss: 0.39579\n",
            "[epoch: 37] loss: 0.40279\n",
            "[epoch: 38] loss: 0.41104\n",
            "[epoch: 39] loss: 0.41992\n",
            "[epoch: 40] loss: 0.42765\n",
            "[epoch: 41] loss: 0.43488\n",
            "[epoch: 42] loss: 0.44435\n",
            "[epoch: 43] loss: 0.45140\n",
            "[epoch: 44] loss: 0.45724\n",
            "[epoch: 45] loss: 0.46627\n",
            "[epoch: 46] loss: 0.47401\n",
            "[epoch: 47] loss: 0.47914\n",
            "[epoch: 48] loss: 0.48712\n",
            "[epoch: 49] loss: 0.49314\n",
            "[epoch: 50] loss: 0.48598\n",
            "[epoch: 51] loss: 0.48649\n",
            "[epoch: 52] loss: 0.49472\n",
            "[epoch: 53] loss: 0.49932\n",
            "[epoch: 54] loss: 0.50617\n",
            "[epoch: 55] loss: 0.51142\n",
            "[epoch: 56] loss: 0.52008\n",
            "[epoch: 57] loss: 0.52253\n",
            "[epoch: 58] loss: 0.53006\n",
            "[epoch: 59] loss: 0.53554\n",
            "[epoch: 60] loss: 0.54099\n",
            "[epoch: 61] loss: 0.54746\n",
            "[epoch: 62] loss: 0.55263\n",
            "[epoch: 63] loss: 0.55857\n",
            "[epoch: 64] loss: 0.56385\n",
            "[epoch: 65] loss: 0.56800\n",
            "[epoch: 66] loss: 0.57290\n",
            "[epoch: 67] loss: 0.57917\n",
            "[epoch: 68] loss: 0.58272\n",
            "[epoch: 69] loss: 0.58850\n",
            "[epoch: 70] loss: 0.59026\n",
            "[epoch: 71] loss: 0.59697\n",
            "[epoch: 72] loss: 0.60263\n",
            "[epoch: 73] loss: 0.60390\n",
            "[epoch: 74] loss: 0.61108\n",
            "[epoch: 75] loss: 0.59065\n",
            "[epoch: 76] loss: 0.58833\n",
            "[epoch: 77] loss: 0.59074\n",
            "[epoch: 78] loss: 0.59472\n",
            "[epoch: 79] loss: 0.59888\n",
            "[epoch: 80] loss: 0.60203\n",
            "[epoch: 81] loss: 0.60460\n",
            "[epoch: 82] loss: 0.60915\n",
            "[epoch: 83] loss: 0.61030\n",
            "[epoch: 84] loss: 0.61864\n",
            "[epoch: 85] loss: 0.61986\n",
            "[epoch: 86] loss: 0.62421\n",
            "[epoch: 87] loss: 0.62793\n",
            "[epoch: 88] loss: 0.62892\n",
            "[epoch: 89] loss: 0.63317\n",
            "[epoch: 90] loss: 0.63441\n",
            "[epoch: 91] loss: 0.63889\n",
            "[epoch: 92] loss: 0.64177\n",
            "[epoch: 93] loss: 0.64178\n",
            "[epoch: 94] loss: 0.64645\n",
            "[epoch: 95] loss: 0.64951\n",
            "[epoch: 96] loss: 0.65113\n",
            "[epoch: 97] loss: 0.65453\n",
            "[epoch: 98] loss: 0.65499\n",
            "[epoch: 99] loss: 0.65518\n",
            "[epoch: 100] loss: 0.63109\n",
            "[epoch: 101] loss: 0.62512\n",
            "[epoch: 102] loss: 0.62649\n",
            "[epoch: 103] loss: 0.62141\n",
            "[epoch: 104] loss: 0.62494\n",
            "[epoch: 105] loss: 0.62360\n",
            "[epoch: 106] loss: 0.62622\n",
            "[epoch: 107] loss: 0.62695\n",
            "[epoch: 108] loss: 0.62581\n",
            "[epoch: 109] loss: 0.62591\n",
            "[epoch: 110] loss: 0.62899\n",
            "[epoch: 111] loss: 0.63211\n",
            "[epoch: 112] loss: 0.63512\n",
            "[epoch: 113] loss: 0.63036\n",
            "[epoch: 114] loss: 0.63619\n",
            "[epoch: 115] loss: 0.63957\n",
            "[epoch: 116] loss: 0.63623\n",
            "[epoch: 117] loss: 0.63539\n",
            "[epoch: 118] loss: 0.64160\n",
            "[epoch: 119] loss: 0.64251\n",
            "[epoch: 120] loss: 0.63972\n",
            "[epoch: 121] loss: 0.64452\n",
            "[epoch: 122] loss: 0.64454\n",
            "[epoch: 123] loss: 0.64120\n",
            "[epoch: 124] loss: 0.64590\n",
            "[epoch: 125] loss: 0.61210\n",
            "[epoch: 126] loss: 0.60055\n",
            "[epoch: 127] loss: 0.59705\n",
            "[epoch: 128] loss: 0.59749\n",
            "[epoch: 129] loss: 0.59409\n",
            "[epoch: 130] loss: 0.59652\n",
            "[epoch: 131] loss: 0.59576\n",
            "[epoch: 132] loss: 0.59285\n",
            "[epoch: 133] loss: 0.59048\n",
            "[epoch: 134] loss: 0.59669\n",
            "[epoch: 135] loss: 0.59712\n",
            "[epoch: 136] loss: 0.58932\n",
            "[epoch: 137] loss: 0.58875\n",
            "[epoch: 138] loss: 0.59353\n",
            "[epoch: 139] loss: 0.59027\n",
            "[epoch: 140] loss: 0.58896\n",
            "[epoch: 141] loss: 0.59155\n",
            "[epoch: 142] loss: 0.59414\n",
            "[epoch: 143] loss: 0.59174\n",
            "[epoch: 144] loss: 0.59023\n",
            "[epoch: 145] loss: 0.59012\n",
            "[epoch: 146] loss: 0.58750\n",
            "[epoch: 147] loss: 0.58944\n",
            "[epoch: 148] loss: 0.59306\n",
            "[epoch: 149] loss: 0.58961\n",
            "[epoch: 150] loss: 0.56573\n",
            "[epoch: 151] loss: 0.55394\n",
            "[epoch: 152] loss: 0.54819\n",
            "[epoch: 153] loss: 0.54322\n",
            "[epoch: 154] loss: 0.54347\n",
            "[epoch: 155] loss: 0.53834\n",
            "[epoch: 156] loss: 0.53296\n",
            "[epoch: 157] loss: 0.53586\n",
            "[epoch: 158] loss: 0.53853\n",
            "[epoch: 159] loss: 0.52995\n",
            "[epoch: 160] loss: 0.53624\n",
            "[epoch: 161] loss: 0.52819\n",
            "[epoch: 162] loss: 0.53217\n",
            "[epoch: 163] loss: 0.52705\n",
            "[epoch: 164] loss: 0.52714\n",
            "[epoch: 165] loss: 0.52937\n",
            "[epoch: 166] loss: 0.52819\n",
            "[epoch: 167] loss: 0.52408\n",
            "[epoch: 168] loss: 0.52093\n",
            "[epoch: 169] loss: 0.52124\n",
            "[epoch: 170] loss: 0.51859\n",
            "[epoch: 171] loss: 0.52253\n",
            "[epoch: 172] loss: 0.51713\n",
            "[epoch: 173] loss: 0.52263\n",
            "[epoch: 174] loss: 0.51831\n",
            "[epoch: 175] loss: 0.49483\n",
            "[epoch: 176] loss: 0.49176\n",
            "[epoch: 177] loss: 0.48897\n",
            "[epoch: 178] loss: 0.48166\n",
            "[epoch: 179] loss: 0.47618\n",
            "[epoch: 180] loss: 0.47667\n",
            "[epoch: 181] loss: 0.47293\n",
            "[epoch: 182] loss: 0.47390\n",
            "[epoch: 183] loss: 0.47090\n",
            "[epoch: 184] loss: 0.47387\n",
            "[epoch: 185] loss: 0.46923\n",
            "[epoch: 186] loss: 0.46598\n",
            "[epoch: 187] loss: 0.46154\n",
            "[epoch: 188] loss: 0.46680\n",
            "[epoch: 189] loss: 0.46519\n",
            "[epoch: 190] loss: 0.45872\n",
            "[epoch: 191] loss: 0.45550\n",
            "[epoch: 192] loss: 0.45826\n",
            "[epoch: 193] loss: 0.45764\n",
            "[epoch: 194] loss: 0.45756\n",
            "[epoch: 195] loss: 0.45769\n",
            "[epoch: 196] loss: 0.45378\n",
            "[epoch: 197] loss: 0.45375\n",
            "[epoch: 198] loss: 0.45331\n",
            "[epoch: 199] loss: 0.45166\n",
            "[epoch: 200] loss: 0.43978\n",
            "[epoch: 201] loss: 0.43340\n",
            "[epoch: 202] loss: 0.42737\n",
            "[epoch: 203] loss: 0.42701\n",
            "[epoch: 204] loss: 0.42353\n",
            "[epoch: 205] loss: 0.42751\n",
            "[epoch: 206] loss: 0.41840\n",
            "[epoch: 207] loss: 0.42175\n",
            "[epoch: 208] loss: 0.41690\n",
            "[epoch: 209] loss: 0.42084\n",
            "[epoch: 210] loss: 0.41683\n",
            "[epoch: 211] loss: 0.41511\n",
            "[epoch: 212] loss: 0.41293\n",
            "[epoch: 213] loss: 0.41311\n",
            "[epoch: 214] loss: 0.41555\n",
            "[epoch: 215] loss: 0.41425\n",
            "[epoch: 216] loss: 0.41037\n",
            "[epoch: 217] loss: 0.41125\n",
            "[epoch: 218] loss: 0.40487\n",
            "[epoch: 219] loss: 0.41043\n",
            "[epoch: 220] loss: 0.40866\n",
            "[epoch: 221] loss: 0.40702\n",
            "[epoch: 222] loss: 0.39971\n",
            "[epoch: 223] loss: 0.40290\n",
            "[epoch: 224] loss: 0.39930\n",
            "[epoch: 225] loss: 0.39124\n",
            "[epoch: 226] loss: 0.38796\n",
            "[epoch: 227] loss: 0.38667\n",
            "[epoch: 228] loss: 0.38994\n",
            "[epoch: 229] loss: 0.38897\n",
            "[epoch: 230] loss: 0.38331\n",
            "[epoch: 231] loss: 0.38492\n",
            "[epoch: 232] loss: 0.38253\n",
            "[epoch: 233] loss: 0.38388\n",
            "[epoch: 234] loss: 0.37970\n",
            "[epoch: 235] loss: 0.38158\n",
            "[epoch: 236] loss: 0.37940\n",
            "[epoch: 237] loss: 0.37667\n",
            "[epoch: 238] loss: 0.37691\n",
            "[epoch: 239] loss: 0.37588\n",
            "[epoch: 240] loss: 0.37658\n",
            "[epoch: 241] loss: 0.37305\n",
            "[epoch: 242] loss: 0.37654\n",
            "[epoch: 243] loss: 0.37623\n",
            "[epoch: 244] loss: 0.37280\n",
            "[epoch: 245] loss: 0.37067\n",
            "[epoch: 246] loss: 0.37026\n",
            "[epoch: 247] loss: 0.37446\n",
            "[epoch: 248] loss: 0.36489\n",
            "[epoch: 249] loss: 0.37298\n",
            "Done!\n",
            "number of abstained set: 6862, number of kept data set: 43138\n",
            "percentage noise filtered: 0.7896\n",
            "percentage noise remained: 0.02438685149983773\n",
            "[epoch: 0] loss: 2.67068\n",
            "[epoch: 1] loss: 1.99096\n",
            "[epoch: 2] loss: 1.90904\n",
            "[epoch: 3] loss: 1.75093\n",
            "[epoch: 4] loss: 1.60454\n",
            "[epoch: 5] loss: 1.46593\n",
            "[epoch: 6] loss: 1.35996\n",
            "[epoch: 7] loss: 1.25814\n",
            "[epoch: 8] loss: 1.19291\n",
            "[epoch: 9] loss: 2.00580\n",
            "[epoch: 10] loss: 1.73598\n",
            "[epoch: 11] loss: 1.60615\n",
            "[epoch: 12] loss: 1.47228\n",
            "[epoch: 13] loss: 1.48172\n",
            "[epoch: 14] loss: 1.98254\n",
            "[epoch: 15] loss: 1.79715\n",
            "[epoch: 16] loss: 1.68778\n",
            "[epoch: 17] loss: 1.56865\n",
            "[epoch: 18] loss: 1.41953\n",
            "[epoch: 19] loss: 1.36776\n",
            "[epoch: 20] loss: 1.25421\n",
            "[epoch: 21] loss: 1.18614\n",
            "[epoch: 22] loss: 1.64762\n",
            "[epoch: 23] loss: 1.32204\n",
            "[epoch: 24] loss: 2.00554\n",
            "[epoch: 25] loss: 1.86352\n",
            "[epoch: 26] loss: 2.07584\n",
            "[epoch: 27] loss: 1.85540\n",
            "[epoch: 28] loss: 1.76597\n",
            "[epoch: 29] loss: 1.75950\n",
            "[epoch: 30] loss: 1.75820\n",
            "[epoch: 31] loss: 1.63500\n",
            "[epoch: 32] loss: 1.58229\n",
            "[epoch: 33] loss: 1.50985\n",
            "[epoch: 34] loss: 1.84748\n",
            "[epoch: 35] loss: 1.71170\n",
            "[epoch: 36] loss: 1.56807\n",
            "[epoch: 37] loss: 1.52657\n",
            "[epoch: 38] loss: 1.64347\n",
            "[epoch: 39] loss: 1.43089\n",
            "[epoch: 40] loss: 1.32935\n",
            "[epoch: 41] loss: 1.25182\n",
            "[epoch: 42] loss: 1.20031\n",
            "[epoch: 43] loss: 1.14518\n",
            "[epoch: 44] loss: 1.18661\n",
            "[epoch: 45] loss: 1.08659\n",
            "[epoch: 46] loss: 1.14088\n",
            "[epoch: 47] loss: 1.02100\n",
            "[epoch: 48] loss: 1.07562\n",
            "[epoch: 49] loss: 1.08811\n",
            "[epoch: 50] loss: 0.90521\n",
            "[epoch: 51] loss: 0.86359\n",
            "[epoch: 52] loss: 0.87751\n",
            "[epoch: 53] loss: 0.84810\n",
            "[epoch: 54] loss: 1.19326\n",
            "[epoch: 55] loss: 1.60111\n",
            "[epoch: 56] loss: 1.21988\n",
            "[epoch: 57] loss: 1.04653\n",
            "[epoch: 58] loss: 1.94380\n",
            "[epoch: 59] loss: 1.64760\n",
            "[epoch: 60] loss: 1.50219\n",
            "[epoch: 61] loss: 1.39082\n",
            "[epoch: 62] loss: 1.31780\n",
            "[epoch: 63] loss: 1.26444\n",
            "[epoch: 64] loss: 1.25973\n",
            "[epoch: 65] loss: 1.11706\n",
            "[epoch: 66] loss: 1.12291\n",
            "[epoch: 67] loss: 1.04445\n",
            "[epoch: 68] loss: 1.05812\n",
            "[epoch: 69] loss: 0.96874\n",
            "[epoch: 70] loss: 0.93106\n",
            "[epoch: 71] loss: 1.56064\n",
            "[epoch: 72] loss: 1.23281\n",
            "[epoch: 73] loss: 1.20397\n",
            "[epoch: 74] loss: 1.09089\n",
            "[epoch: 75] loss: 0.95686\n",
            "[epoch: 76] loss: 0.96072\n",
            "[epoch: 77] loss: 1.15875\n",
            "[epoch: 78] loss: 0.96704\n",
            "[epoch: 79] loss: 0.91563\n",
            "[epoch: 80] loss: 0.91481\n",
            "[epoch: 81] loss: 0.85398\n",
            "[epoch: 82] loss: 0.83146\n",
            "[epoch: 83] loss: 0.91791\n",
            "[epoch: 84] loss: 0.81507\n",
            "[epoch: 85] loss: 0.79645\n",
            "[epoch: 86] loss: 0.79146\n",
            "[epoch: 87] loss: 0.76412\n",
            "[epoch: 88] loss: 1.01385\n",
            "[epoch: 89] loss: 0.94836\n",
            "[epoch: 90] loss: 0.90254\n",
            "[epoch: 91] loss: 0.79895\n",
            "[epoch: 92] loss: 0.75759\n",
            "[epoch: 93] loss: 0.72850\n",
            "[epoch: 94] loss: 0.79829\n",
            "[epoch: 95] loss: 1.14060\n",
            "[epoch: 96] loss: 0.86261\n",
            "[epoch: 97] loss: 0.78612\n",
            "[epoch: 98] loss: 0.89310\n",
            "[epoch: 99] loss: 0.77154\n",
            "[epoch: 100] loss: 0.77151\n",
            "[epoch: 101] loss: 0.70936\n",
            "[epoch: 102] loss: 0.68711\n",
            "[epoch: 103] loss: 0.77296\n",
            "[epoch: 104] loss: 0.79496\n",
            "[epoch: 105] loss: 0.71881\n",
            "[epoch: 106] loss: 0.66777\n",
            "[epoch: 107] loss: 0.64947\n",
            "[epoch: 108] loss: 0.64539\n",
            "[epoch: 109] loss: 0.63043\n",
            "[epoch: 110] loss: 0.65590\n",
            "[epoch: 111] loss: 0.62265\n",
            "[epoch: 112] loss: 0.70605\n",
            "[epoch: 113] loss: 0.62208\n",
            "[epoch: 114] loss: 0.60582\n",
            "[epoch: 115] loss: 0.60394\n",
            "[epoch: 116] loss: 0.59768\n",
            "[epoch: 117] loss: 0.68334\n",
            "[epoch: 118] loss: 0.62199\n",
            "[epoch: 119] loss: 0.59060\n",
            "[epoch: 120] loss: 0.61789\n",
            "[epoch: 121] loss: 0.58643\n",
            "[epoch: 122] loss: 0.58350\n",
            "[epoch: 123] loss: 0.64206\n",
            "[epoch: 124] loss: 0.80796\n",
            "[epoch: 125] loss: 0.62370\n",
            "[epoch: 126] loss: 0.60173\n",
            "[epoch: 127] loss: 0.58425\n",
            "[epoch: 128] loss: 0.56600\n",
            "[epoch: 129] loss: 0.56093\n",
            "[epoch: 130] loss: 0.54342\n",
            "[epoch: 131] loss: 0.53623\n",
            "[epoch: 132] loss: 0.54538\n",
            "[epoch: 133] loss: 0.52483\n",
            "[epoch: 134] loss: 0.53941\n",
            "[epoch: 135] loss: 0.51745\n",
            "[epoch: 136] loss: 0.51313\n",
            "[epoch: 137] loss: 0.51264\n",
            "[epoch: 138] loss: 0.51215\n",
            "[epoch: 139] loss: 0.49822\n",
            "[epoch: 140] loss: 0.55673\n",
            "[epoch: 141] loss: 0.51300\n",
            "[epoch: 142] loss: 0.49511\n",
            "[epoch: 143] loss: 0.48766\n",
            "[epoch: 144] loss: 0.48682\n",
            "[epoch: 145] loss: 0.47624\n",
            "[epoch: 146] loss: 0.46910\n",
            "[epoch: 147] loss: 0.46445\n",
            "[epoch: 148] loss: 0.46021\n",
            "[epoch: 149] loss: 0.45697\n",
            "[epoch: 150] loss: 0.43831\n",
            "[epoch: 151] loss: 0.43004\n",
            "[epoch: 152] loss: 0.42533\n",
            "[epoch: 153] loss: 0.42788\n",
            "[epoch: 154] loss: 0.41726\n",
            "[epoch: 155] loss: 0.42300\n",
            "[epoch: 156] loss: 0.43436\n",
            "[epoch: 157] loss: 0.42007\n",
            "[epoch: 158] loss: 0.41915\n",
            "[epoch: 159] loss: 0.41057\n",
            "[epoch: 160] loss: 0.40699\n",
            "[epoch: 161] loss: 0.42146\n",
            "[epoch: 162] loss: 0.43571\n",
            "[epoch: 163] loss: 0.41683\n",
            "[epoch: 164] loss: 0.41695\n",
            "[epoch: 165] loss: 0.41353\n",
            "[epoch: 166] loss: 0.39370\n",
            "[epoch: 167] loss: 0.39226\n",
            "[epoch: 168] loss: 0.39083\n",
            "[epoch: 169] loss: 0.39110\n",
            "[epoch: 170] loss: 0.44393\n",
            "[epoch: 171] loss: 0.41590\n",
            "[epoch: 172] loss: 0.40866\n",
            "[epoch: 173] loss: 0.39123\n",
            "[epoch: 174] loss: 0.39361\n",
            "[epoch: 175] loss: 0.38557\n",
            "[epoch: 176] loss: 0.37666\n",
            "[epoch: 177] loss: 0.37691\n",
            "[epoch: 178] loss: 0.38022\n",
            "[epoch: 179] loss: 0.37093\n",
            "[epoch: 180] loss: 0.38267\n",
            "[epoch: 181] loss: 0.37473\n",
            "[epoch: 182] loss: 0.37024\n",
            "[epoch: 183] loss: 0.49233\n",
            "[epoch: 184] loss: 0.40435\n",
            "[epoch: 185] loss: 0.39286\n",
            "[epoch: 186] loss: 0.40174\n",
            "[epoch: 187] loss: 0.37858\n",
            "[epoch: 188] loss: 0.37522\n",
            "[epoch: 189] loss: 0.36680\n",
            "[epoch: 190] loss: 0.38515\n",
            "[epoch: 191] loss: 0.38175\n",
            "[epoch: 192] loss: 0.39148\n",
            "[epoch: 193] loss: 0.37571\n",
            "[epoch: 194] loss: 0.37737\n",
            "[epoch: 195] loss: 0.36768\n",
            "[epoch: 196] loss: 0.36376\n",
            "[epoch: 197] loss: 0.38916\n",
            "[epoch: 198] loss: 0.36661\n",
            "[epoch: 199] loss: 0.36200\n",
            "[epoch: 200] loss: 0.35000\n",
            "[epoch: 201] loss: 0.34857\n",
            "[epoch: 202] loss: 0.34648\n",
            "[epoch: 203] loss: 0.35832\n",
            "[epoch: 204] loss: 0.35098\n",
            "[epoch: 205] loss: 0.35249\n",
            "[epoch: 206] loss: 0.34565\n",
            "[epoch: 207] loss: 0.35028\n",
            "[epoch: 208] loss: 0.34977\n",
            "[epoch: 209] loss: 0.34547\n",
            "[epoch: 210] loss: 0.36256\n",
            "[epoch: 211] loss: 0.34284\n",
            "[epoch: 212] loss: 0.33762\n",
            "[epoch: 213] loss: 0.34112\n",
            "[epoch: 214] loss: 0.33624\n",
            "[epoch: 215] loss: 0.34722\n",
            "[epoch: 216] loss: 0.33498\n",
            "[epoch: 217] loss: 0.33628\n",
            "[epoch: 218] loss: 0.32990\n",
            "[epoch: 219] loss: 0.33206\n",
            "[epoch: 220] loss: 0.32630\n",
            "[epoch: 221] loss: 0.32866\n",
            "[epoch: 222] loss: 0.32504\n",
            "[epoch: 223] loss: 0.32259\n",
            "[epoch: 224] loss: 0.32724\n",
            "[epoch: 225] loss: 0.32137\n",
            "[epoch: 226] loss: 0.32260\n",
            "[epoch: 227] loss: 0.32257\n",
            "[epoch: 228] loss: 0.32356\n",
            "[epoch: 229] loss: 0.31950\n",
            "[epoch: 230] loss: 0.32708\n",
            "[epoch: 231] loss: 0.32846\n",
            "[epoch: 232] loss: 0.31743\n",
            "[epoch: 233] loss: 0.32148\n",
            "[epoch: 234] loss: 0.32683\n",
            "[epoch: 235] loss: 0.31909\n",
            "[epoch: 236] loss: 0.31385\n",
            "[epoch: 237] loss: 0.30840\n",
            "[epoch: 238] loss: 0.32123\n",
            "[epoch: 239] loss: 0.31139\n",
            "[epoch: 240] loss: 0.31213\n",
            "[epoch: 241] loss: 0.30975\n",
            "[epoch: 242] loss: 0.30713\n",
            "[epoch: 243] loss: 0.30791\n",
            "[epoch: 244] loss: 0.30971\n",
            "[epoch: 245] loss: 0.30980\n",
            "[epoch: 246] loss: 0.30604\n",
            "[epoch: 247] loss: 0.30714\n",
            "[epoch: 248] loss: 0.32634\n",
            "[epoch: 249] loss: 0.31525\n",
            "Accuracy of the network on the 10000 test images: 83.12 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Training dac and post_dac at noise level 0.2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "using dac loss function\n",
            "\n",
            "[epoch: 0] loss: 2.79213\n",
            "[epoch: 1] loss: 2.16119\n",
            "[epoch: 2] loss: 2.09691\n",
            "[epoch: 3] loss: 2.03308\n",
            "[epoch: 4] loss: 1.98242\n",
            "[epoch: 5] loss: 1.93823\n",
            "[epoch: 6] loss: 1.88337\n",
            "[epoch: 7] loss: 1.81667\n",
            "[epoch: 8] loss: 1.75555\n",
            "[epoch: 9] loss: 1.70473\n",
            "[epoch: 10] loss: 1.66579\n",
            "[epoch: 11] loss: 0.60395\n",
            "[epoch: 12] loss: 0.14915\n",
            "[epoch: 13] loss: 0.16338\n",
            "[epoch: 14] loss: 0.17776\n",
            "[epoch: 15] loss: 0.19194\n",
            "[epoch: 16] loss: 0.20608\n",
            "[epoch: 17] loss: 0.22048\n",
            "[epoch: 18] loss: 0.23485\n",
            "[epoch: 19] loss: 0.24885\n",
            "[epoch: 20] loss: 0.26277\n",
            "[epoch: 21] loss: 0.27618\n",
            "[epoch: 22] loss: 0.28849\n",
            "[epoch: 23] loss: 0.30085\n",
            "[epoch: 24] loss: 0.31326\n",
            "[epoch: 25] loss: 0.31986\n",
            "[epoch: 26] loss: 0.33073\n",
            "[epoch: 27] loss: 0.34161\n",
            "[epoch: 28] loss: 0.35294\n",
            "[epoch: 29] loss: 0.36335\n",
            "[epoch: 30] loss: 0.37393\n",
            "[epoch: 31] loss: 0.38438\n",
            "[epoch: 32] loss: 0.39471\n",
            "[epoch: 33] loss: 0.40442\n",
            "[epoch: 34] loss: 0.41457\n",
            "[epoch: 35] loss: 0.42414\n",
            "[epoch: 36] loss: 0.43371\n",
            "[epoch: 37] loss: 0.44308\n",
            "[epoch: 38] loss: 0.45248\n",
            "[epoch: 39] loss: 0.46129\n",
            "[epoch: 40] loss: 0.47029\n",
            "[epoch: 41] loss: 0.47872\n",
            "[epoch: 42] loss: 0.48744\n",
            "[epoch: 43] loss: 0.49702\n",
            "[epoch: 44] loss: 0.50406\n",
            "[epoch: 45] loss: 0.51362\n",
            "[epoch: 46] loss: 0.52143\n",
            "[epoch: 47] loss: 0.52987\n",
            "[epoch: 48] loss: 0.53818\n",
            "[epoch: 49] loss: 0.54509\n",
            "[epoch: 50] loss: 0.54364\n",
            "[epoch: 51] loss: 0.54917\n",
            "[epoch: 52] loss: 0.55601\n",
            "[epoch: 53] loss: 0.56395\n",
            "[epoch: 54] loss: 0.57107\n",
            "[epoch: 55] loss: 0.57867\n",
            "[epoch: 56] loss: 0.58647\n",
            "[epoch: 57] loss: 0.59262\n",
            "[epoch: 58] loss: 0.60078\n",
            "[epoch: 59] loss: 0.60819\n",
            "[epoch: 60] loss: 0.61422\n",
            "[epoch: 61] loss: 0.62147\n",
            "[epoch: 62] loss: 0.62723\n",
            "[epoch: 63] loss: 0.63342\n",
            "[epoch: 64] loss: 0.63965\n",
            "[epoch: 65] loss: 0.64771\n",
            "[epoch: 66] loss: 0.65367\n",
            "[epoch: 67] loss: 0.66010\n",
            "[epoch: 68] loss: 0.66610\n",
            "[epoch: 69] loss: 0.67298\n",
            "[epoch: 70] loss: 0.67807\n",
            "[epoch: 71] loss: 0.68406\n",
            "[epoch: 72] loss: 0.69019\n",
            "[epoch: 73] loss: 0.69569\n",
            "[epoch: 74] loss: 0.70229\n",
            "[epoch: 75] loss: 0.69375\n",
            "[epoch: 76] loss: 0.69521\n",
            "[epoch: 77] loss: 0.69970\n",
            "[epoch: 78] loss: 0.70447\n",
            "[epoch: 79] loss: 0.71249\n",
            "[epoch: 80] loss: 0.71473\n",
            "[epoch: 81] loss: 0.72204\n",
            "[epoch: 82] loss: 0.72816\n",
            "[epoch: 83] loss: 0.73153\n",
            "[epoch: 84] loss: 0.73858\n",
            "[epoch: 85] loss: 0.74245\n",
            "[epoch: 86] loss: 0.74701\n",
            "[epoch: 87] loss: 0.75144\n",
            "[epoch: 88] loss: 0.75571\n",
            "[epoch: 89] loss: 0.76379\n",
            "[epoch: 90] loss: 0.76755\n",
            "[epoch: 91] loss: 0.77055\n",
            "[epoch: 92] loss: 0.77689\n",
            "[epoch: 93] loss: 0.78057\n",
            "[epoch: 94] loss: 0.78399\n",
            "[epoch: 95] loss: 0.78786\n",
            "[epoch: 96] loss: 0.79507\n",
            "[epoch: 97] loss: 0.79756\n",
            "[epoch: 98] loss: 0.80260\n",
            "[epoch: 99] loss: 0.80704\n",
            "[epoch: 100] loss: 0.79241\n",
            "[epoch: 101] loss: 0.78868\n",
            "[epoch: 102] loss: 0.79291\n",
            "[epoch: 103] loss: 0.79385\n",
            "[epoch: 104] loss: 0.80039\n",
            "[epoch: 105] loss: 0.80085\n",
            "[epoch: 106] loss: 0.80399\n",
            "[epoch: 107] loss: 0.80710\n",
            "[epoch: 108] loss: 0.81054\n",
            "[epoch: 109] loss: 0.81303\n",
            "[epoch: 110] loss: 0.81941\n",
            "[epoch: 111] loss: 0.82011\n",
            "[epoch: 112] loss: 0.82374\n",
            "[epoch: 113] loss: 0.83081\n",
            "[epoch: 114] loss: 0.82932\n",
            "[epoch: 115] loss: 0.83206\n",
            "[epoch: 116] loss: 0.83087\n",
            "[epoch: 117] loss: 0.83862\n",
            "[epoch: 118] loss: 0.84325\n",
            "[epoch: 119] loss: 0.84315\n",
            "[epoch: 120] loss: 0.84548\n",
            "[epoch: 121] loss: 0.84730\n",
            "[epoch: 122] loss: 0.85228\n",
            "[epoch: 123] loss: 0.85301\n",
            "[epoch: 124] loss: 0.85780\n",
            "[epoch: 125] loss: 0.82979\n",
            "[epoch: 126] loss: 0.82135\n",
            "[epoch: 127] loss: 0.82315\n",
            "[epoch: 128] loss: 0.82245\n",
            "[epoch: 129] loss: 0.82345\n",
            "[epoch: 130] loss: 0.81977\n",
            "[epoch: 131] loss: 0.82406\n",
            "[epoch: 132] loss: 0.82240\n",
            "[epoch: 133] loss: 0.82622\n",
            "[epoch: 134] loss: 0.82813\n",
            "[epoch: 135] loss: 0.82679\n",
            "[epoch: 136] loss: 0.83004\n",
            "[epoch: 137] loss: 0.83085\n",
            "[epoch: 138] loss: 0.83234\n",
            "[epoch: 139] loss: 0.83288\n",
            "[epoch: 140] loss: 0.83257\n",
            "[epoch: 141] loss: 0.83476\n",
            "[epoch: 142] loss: 0.83445\n",
            "[epoch: 143] loss: 0.83811\n",
            "[epoch: 144] loss: 0.83315\n",
            "[epoch: 145] loss: 0.83868\n",
            "[epoch: 146] loss: 0.83496\n",
            "[epoch: 147] loss: 0.83764\n",
            "[epoch: 148] loss: 0.83464\n",
            "[epoch: 149] loss: 0.84169\n",
            "[epoch: 150] loss: 0.81574\n",
            "[epoch: 151] loss: 0.79870\n",
            "[epoch: 152] loss: 0.79381\n",
            "[epoch: 153] loss: 0.79263\n",
            "[epoch: 154] loss: 0.79563\n",
            "[epoch: 155] loss: 0.79135\n",
            "[epoch: 156] loss: 0.78907\n",
            "[epoch: 157] loss: 0.79349\n",
            "[epoch: 158] loss: 0.78999\n",
            "[epoch: 159] loss: 0.79083\n",
            "[epoch: 160] loss: 0.78914\n",
            "[epoch: 161] loss: 0.79232\n",
            "[epoch: 162] loss: 0.79165\n",
            "[epoch: 163] loss: 0.78783\n",
            "[epoch: 164] loss: 0.79004\n",
            "[epoch: 165] loss: 0.78696\n",
            "[epoch: 166] loss: 0.78918\n",
            "[epoch: 167] loss: 0.79108\n",
            "[epoch: 168] loss: 0.78763\n",
            "[epoch: 169] loss: 0.78401\n",
            "[epoch: 170] loss: 0.78435\n",
            "[epoch: 171] loss: 0.78998\n",
            "[epoch: 172] loss: 0.78979\n",
            "[epoch: 173] loss: 0.78561\n",
            "[epoch: 174] loss: 0.78958\n",
            "[epoch: 175] loss: 0.75615\n",
            "[epoch: 176] loss: 0.75240\n",
            "[epoch: 177] loss: 0.74976\n",
            "[epoch: 178] loss: 0.74463\n",
            "[epoch: 179] loss: 0.74555\n",
            "[epoch: 180] loss: 0.73736\n",
            "[epoch: 181] loss: 0.73933\n",
            "[epoch: 182] loss: 0.73865\n",
            "[epoch: 183] loss: 0.73955\n",
            "[epoch: 184] loss: 0.73538\n",
            "[epoch: 185] loss: 0.73620\n",
            "[epoch: 186] loss: 0.73277\n",
            "[epoch: 187] loss: 0.73879\n",
            "[epoch: 188] loss: 0.73234\n",
            "[epoch: 189] loss: 0.73742\n",
            "[epoch: 190] loss: 0.73231\n",
            "[epoch: 191] loss: 0.73245\n",
            "[epoch: 192] loss: 0.73090\n",
            "[epoch: 193] loss: 0.72854\n",
            "[epoch: 194] loss: 0.73198\n",
            "[epoch: 195] loss: 0.73142\n",
            "[epoch: 196] loss: 0.72717\n",
            "[epoch: 197] loss: 0.72615\n",
            "[epoch: 198] loss: 0.72798\n",
            "[epoch: 199] loss: 0.72594\n",
            "[epoch: 200] loss: 0.70735\n",
            "[epoch: 201] loss: 0.70333\n",
            "[epoch: 202] loss: 0.69296\n",
            "[epoch: 203] loss: 0.69609\n",
            "[epoch: 204] loss: 0.69394\n",
            "[epoch: 205] loss: 0.69456\n",
            "[epoch: 206] loss: 0.69146\n",
            "[epoch: 207] loss: 0.68925\n",
            "[epoch: 208] loss: 0.68862\n",
            "[epoch: 209] loss: 0.69011\n",
            "[epoch: 210] loss: 0.68786\n",
            "[epoch: 211] loss: 0.68494\n",
            "[epoch: 212] loss: 0.68605\n",
            "[epoch: 213] loss: 0.68355\n",
            "[epoch: 214] loss: 0.68059\n",
            "[epoch: 215] loss: 0.68026\n",
            "[epoch: 216] loss: 0.68005\n",
            "[epoch: 217] loss: 0.68599\n",
            "[epoch: 218] loss: 0.67958\n",
            "[epoch: 219] loss: 0.67723\n",
            "[epoch: 220] loss: 0.67829\n",
            "[epoch: 221] loss: 0.68077\n",
            "[epoch: 222] loss: 0.68215\n",
            "[epoch: 223] loss: 0.67244\n",
            "[epoch: 224] loss: 0.67335\n",
            "[epoch: 225] loss: 0.66802\n",
            "[epoch: 226] loss: 0.66537\n",
            "[epoch: 227] loss: 0.65929\n",
            "[epoch: 228] loss: 0.65843\n",
            "[epoch: 229] loss: 0.65420\n",
            "[epoch: 230] loss: 0.65417\n",
            "[epoch: 231] loss: 0.65293\n",
            "[epoch: 232] loss: 0.65224\n",
            "[epoch: 233] loss: 0.65805\n",
            "[epoch: 234] loss: 0.65631\n",
            "[epoch: 235] loss: 0.65238\n",
            "[epoch: 236] loss: 0.64368\n",
            "[epoch: 237] loss: 0.64888\n",
            "[epoch: 238] loss: 0.65047\n",
            "[epoch: 239] loss: 0.64745\n",
            "[epoch: 240] loss: 0.64177\n",
            "[epoch: 241] loss: 0.64332\n",
            "[epoch: 242] loss: 0.65137\n",
            "[epoch: 243] loss: 0.64240\n",
            "[epoch: 244] loss: 0.64482\n",
            "[epoch: 245] loss: 0.64522\n",
            "[epoch: 246] loss: 0.64338\n",
            "[epoch: 247] loss: 0.64235\n",
            "[epoch: 248] loss: 0.64565\n",
            "[epoch: 249] loss: 0.63952\n",
            "Done!\n",
            "number of abstained set: 13718, number of kept data set: 36282\n",
            "percentage noise filtered: 0.8835\n",
            "percentage noise remained: 0.03210958602061628\n",
            "[epoch: 0] loss: 2.81468\n",
            "[epoch: 1] loss: 1.97276\n",
            "[epoch: 2] loss: 1.80702\n",
            "[epoch: 3] loss: 1.69055\n",
            "[epoch: 4] loss: 1.59752\n",
            "[epoch: 5] loss: 1.47317\n",
            "[epoch: 6] loss: 1.34615\n",
            "[epoch: 7] loss: 1.24432\n",
            "[epoch: 8] loss: 1.15695\n",
            "[epoch: 9] loss: 1.09388\n",
            "[epoch: 10] loss: 1.05048\n",
            "[epoch: 11] loss: 1.00578\n",
            "[epoch: 12] loss: 0.97712\n",
            "[epoch: 13] loss: 0.95471\n",
            "[epoch: 14] loss: 0.92958\n",
            "[epoch: 15] loss: 0.91397\n",
            "[epoch: 16] loss: 0.90931\n",
            "[epoch: 17] loss: 0.88443\n",
            "[epoch: 18] loss: 0.87089\n",
            "[epoch: 19] loss: 0.85272\n",
            "[epoch: 20] loss: 0.83527\n",
            "[epoch: 21] loss: 0.83130\n",
            "[epoch: 22] loss: 0.81827\n",
            "[epoch: 23] loss: 0.80477\n",
            "[epoch: 24] loss: 0.80183\n",
            "[epoch: 25] loss: 0.67388\n",
            "[epoch: 26] loss: 0.65345\n",
            "[epoch: 27] loss: 0.63849\n",
            "[epoch: 28] loss: 0.62877\n",
            "[epoch: 29] loss: 0.63164\n",
            "[epoch: 30] loss: 0.63512\n",
            "[epoch: 31] loss: 0.62694\n",
            "[epoch: 32] loss: 0.63164\n",
            "[epoch: 33] loss: 0.62634\n",
            "[epoch: 34] loss: 0.62351\n",
            "[epoch: 35] loss: 0.62720\n",
            "[epoch: 36] loss: 0.61488\n",
            "[epoch: 37] loss: 0.60879\n",
            "[epoch: 38] loss: 0.60859\n",
            "[epoch: 39] loss: 0.60921\n",
            "[epoch: 40] loss: 0.60242\n",
            "[epoch: 41] loss: 0.59732\n",
            "[epoch: 42] loss: 0.59312\n",
            "[epoch: 43] loss: 0.59926\n",
            "[epoch: 44] loss: 0.59081\n",
            "[epoch: 45] loss: 0.58539\n",
            "[epoch: 46] loss: 0.58853\n",
            "[epoch: 47] loss: 0.58458\n",
            "[epoch: 48] loss: 0.57983\n",
            "[epoch: 49] loss: 0.57960\n",
            "[epoch: 50] loss: 0.48724\n",
            "[epoch: 51] loss: 0.46285\n",
            "[epoch: 52] loss: 0.46173\n",
            "[epoch: 53] loss: 0.44273\n",
            "[epoch: 54] loss: 0.44503\n",
            "[epoch: 55] loss: 0.44546\n",
            "[epoch: 56] loss: 0.44677\n",
            "[epoch: 57] loss: 0.44613\n",
            "[epoch: 58] loss: 0.45139\n",
            "[epoch: 59] loss: 0.44399\n",
            "[epoch: 60] loss: 0.44651\n",
            "[epoch: 61] loss: 0.44788\n",
            "[epoch: 62] loss: 0.45059\n",
            "[epoch: 63] loss: 0.44202\n",
            "[epoch: 64] loss: 0.44022\n",
            "[epoch: 65] loss: 0.44885\n",
            "[epoch: 66] loss: 0.43955\n",
            "[epoch: 67] loss: 0.44215\n",
            "[epoch: 68] loss: 0.43878\n",
            "[epoch: 69] loss: 0.44618\n",
            "[epoch: 70] loss: 0.43756\n",
            "[epoch: 71] loss: 0.43790\n",
            "[epoch: 72] loss: 0.43417\n",
            "[epoch: 73] loss: 0.43147\n",
            "[epoch: 74] loss: 0.43676\n",
            "[epoch: 75] loss: 0.36875\n",
            "[epoch: 76] loss: 0.34460\n",
            "[epoch: 77] loss: 0.33485\n",
            "[epoch: 78] loss: 0.33275\n",
            "[epoch: 79] loss: 0.32720\n",
            "[epoch: 80] loss: 0.31651\n",
            "[epoch: 81] loss: 0.32137\n",
            "[epoch: 82] loss: 0.31502\n",
            "[epoch: 83] loss: 0.32513\n",
            "[epoch: 84] loss: 0.32429\n",
            "[epoch: 85] loss: 0.31636\n",
            "[epoch: 86] loss: 0.31837\n",
            "[epoch: 87] loss: 0.31733\n",
            "[epoch: 88] loss: 0.31512\n",
            "[epoch: 89] loss: 0.31939\n",
            "[epoch: 90] loss: 0.31848\n",
            "[epoch: 91] loss: 0.31687\n",
            "[epoch: 92] loss: 0.31175\n",
            "[epoch: 93] loss: 0.31527\n",
            "[epoch: 94] loss: 0.31502\n",
            "[epoch: 95] loss: 0.31687\n",
            "[epoch: 96] loss: 0.31052\n",
            "[epoch: 97] loss: 0.31151\n",
            "[epoch: 98] loss: 0.31713\n",
            "[epoch: 99] loss: 0.30945\n",
            "[epoch: 100] loss: 0.26429\n",
            "[epoch: 101] loss: 0.23974\n",
            "[epoch: 102] loss: 0.23119\n",
            "[epoch: 103] loss: 0.22821\n",
            "[epoch: 104] loss: 0.22683\n",
            "[epoch: 105] loss: 0.22568\n",
            "[epoch: 106] loss: 0.22225\n",
            "[epoch: 107] loss: 0.22200\n",
            "[epoch: 108] loss: 0.21726\n",
            "[epoch: 109] loss: 0.21878\n",
            "[epoch: 110] loss: 0.22533\n",
            "[epoch: 111] loss: 0.21666\n",
            "[epoch: 112] loss: 0.21348\n",
            "[epoch: 113] loss: 0.21722\n",
            "[epoch: 114] loss: 0.21534\n",
            "[epoch: 115] loss: 0.21332\n",
            "[epoch: 116] loss: 0.21065\n",
            "[epoch: 117] loss: 0.21314\n",
            "[epoch: 118] loss: 0.20818\n",
            "[epoch: 119] loss: 0.21626\n",
            "[epoch: 120] loss: 0.21095\n",
            "[epoch: 121] loss: 0.21215\n",
            "[epoch: 122] loss: 0.20506\n",
            "[epoch: 123] loss: 0.20278\n",
            "[epoch: 124] loss: 0.20699\n",
            "[epoch: 125] loss: 0.17255\n",
            "[epoch: 126] loss: 0.16130\n",
            "[epoch: 127] loss: 0.15720\n",
            "[epoch: 128] loss: 0.15490\n",
            "[epoch: 129] loss: 0.14891\n",
            "[epoch: 130] loss: 0.14939\n",
            "[epoch: 131] loss: 0.14801\n",
            "[epoch: 132] loss: 0.14753\n",
            "[epoch: 133] loss: 0.14828\n",
            "[epoch: 134] loss: 0.15029\n",
            "[epoch: 135] loss: 0.14565\n",
            "[epoch: 136] loss: 0.13693\n",
            "[epoch: 137] loss: 0.14402\n",
            "[epoch: 138] loss: 0.13698\n",
            "[epoch: 139] loss: 0.13215\n",
            "[epoch: 140] loss: 0.13238\n",
            "[epoch: 141] loss: 0.13545\n",
            "[epoch: 142] loss: 0.13380\n",
            "[epoch: 143] loss: 0.13842\n",
            "[epoch: 144] loss: 0.13665\n",
            "[epoch: 145] loss: 0.12709\n",
            "[epoch: 146] loss: 0.13675\n",
            "[epoch: 147] loss: 0.13068\n",
            "[epoch: 148] loss: 0.12891\n",
            "[epoch: 149] loss: 0.12845\n",
            "[epoch: 150] loss: 0.11630\n",
            "[epoch: 151] loss: 0.10517\n",
            "[epoch: 152] loss: 0.10016\n",
            "[epoch: 153] loss: 0.10212\n",
            "[epoch: 154] loss: 0.09947\n",
            "[epoch: 155] loss: 0.09934\n",
            "[epoch: 156] loss: 0.09853\n",
            "[epoch: 157] loss: 0.09823\n",
            "[epoch: 158] loss: 0.09101\n",
            "[epoch: 159] loss: 0.09223\n",
            "[epoch: 160] loss: 0.09319\n",
            "[epoch: 161] loss: 0.09069\n",
            "[epoch: 162] loss: 0.09399\n",
            "[epoch: 163] loss: 0.09058\n",
            "[epoch: 164] loss: 0.09107\n",
            "[epoch: 165] loss: 0.09045\n",
            "[epoch: 166] loss: 0.08886\n",
            "[epoch: 167] loss: 0.08931\n",
            "[epoch: 168] loss: 0.08365\n",
            "[epoch: 169] loss: 0.09108\n",
            "[epoch: 170] loss: 0.08909\n",
            "[epoch: 171] loss: 0.09373\n",
            "[epoch: 172] loss: 0.08837\n",
            "[epoch: 173] loss: 0.09215\n",
            "[epoch: 174] loss: 0.08825\n",
            "[epoch: 175] loss: 0.07966\n",
            "[epoch: 176] loss: 0.08039\n",
            "[epoch: 177] loss: 0.07886\n",
            "[epoch: 178] loss: 0.07178\n",
            "[epoch: 179] loss: 0.07490\n",
            "[epoch: 180] loss: 0.07254\n",
            "[epoch: 181] loss: 0.07313\n",
            "[epoch: 182] loss: 0.07341\n",
            "[epoch: 183] loss: 0.06957\n",
            "[epoch: 184] loss: 0.07299\n",
            "[epoch: 185] loss: 0.06902\n",
            "[epoch: 186] loss: 0.07077\n",
            "[epoch: 187] loss: 0.06720\n",
            "[epoch: 188] loss: 0.07014\n",
            "[epoch: 189] loss: 0.06348\n",
            "[epoch: 190] loss: 0.06501\n",
            "[epoch: 191] loss: 0.06561\n",
            "[epoch: 192] loss: 0.06432\n",
            "[epoch: 193] loss: 0.06207\n",
            "[epoch: 194] loss: 0.06585\n",
            "[epoch: 195] loss: 0.06456\n",
            "[epoch: 196] loss: 0.06433\n",
            "[epoch: 197] loss: 0.06531\n",
            "[epoch: 198] loss: 0.06338\n",
            "[epoch: 199] loss: 0.06081\n",
            "[epoch: 200] loss: 0.05803\n",
            "[epoch: 201] loss: 0.05725\n",
            "[epoch: 202] loss: 0.05924\n",
            "[epoch: 203] loss: 0.05829\n",
            "[epoch: 204] loss: 0.05890\n",
            "[epoch: 205] loss: 0.05852\n",
            "[epoch: 206] loss: 0.05700\n",
            "[epoch: 207] loss: 0.05559\n",
            "[epoch: 208] loss: 0.05671\n",
            "[epoch: 209] loss: 0.05908\n",
            "[epoch: 210] loss: 0.05564\n",
            "[epoch: 211] loss: 0.05729\n",
            "[epoch: 212] loss: 0.05286\n",
            "[epoch: 213] loss: 0.05774\n",
            "[epoch: 214] loss: 0.05279\n",
            "[epoch: 215] loss: 0.05411\n",
            "[epoch: 216] loss: 0.05290\n",
            "[epoch: 217] loss: 0.05501\n",
            "[epoch: 218] loss: 0.05289\n",
            "[epoch: 219] loss: 0.05446\n",
            "[epoch: 220] loss: 0.05653\n",
            "[epoch: 221] loss: 0.05446\n",
            "[epoch: 222] loss: 0.05187\n",
            "[epoch: 223] loss: 0.05305\n",
            "[epoch: 224] loss: 0.05539\n",
            "[epoch: 225] loss: 0.05270\n",
            "[epoch: 226] loss: 0.05053\n",
            "[epoch: 227] loss: 0.05155\n",
            "[epoch: 228] loss: 0.05149\n",
            "[epoch: 229] loss: 0.05208\n",
            "[epoch: 230] loss: 0.05134\n",
            "[epoch: 231] loss: 0.04877\n",
            "[epoch: 232] loss: 0.05195\n",
            "[epoch: 233] loss: 0.05032\n",
            "[epoch: 234] loss: 0.04931\n",
            "[epoch: 235] loss: 0.04907\n",
            "[epoch: 236] loss: 0.05023\n",
            "[epoch: 237] loss: 0.04923\n",
            "[epoch: 238] loss: 0.05132\n",
            "[epoch: 239] loss: 0.04939\n",
            "[epoch: 240] loss: 0.05026\n",
            "[epoch: 241] loss: 0.04547\n",
            "[epoch: 242] loss: 0.05008\n",
            "[epoch: 243] loss: 0.04679\n",
            "[epoch: 244] loss: 0.04807\n",
            "[epoch: 245] loss: 0.04926\n",
            "[epoch: 246] loss: 0.04681\n",
            "[epoch: 247] loss: 0.05107\n",
            "[epoch: 248] loss: 0.04645\n",
            "[epoch: 249] loss: 0.04765\n",
            "Accuracy of the network on the 10000 test images: 85.4 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Training dac and post_dac at noise level 0.4\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "using dac loss function\n",
            "\n",
            "[epoch: 0] loss: 2.82895\n",
            "[epoch: 1] loss: 2.29107\n",
            "[epoch: 2] loss: 2.25130\n",
            "[epoch: 3] loss: 2.23106\n",
            "[epoch: 4] loss: 2.21203\n",
            "[epoch: 5] loss: 2.19547\n",
            "[epoch: 6] loss: 2.18707\n",
            "[epoch: 7] loss: 2.17457\n",
            "[epoch: 8] loss: 2.16129\n",
            "[epoch: 9] loss: 2.14773\n",
            "[epoch: 10] loss: 2.13939\n",
            "[epoch: 11] loss: 0.65832\n",
            "[epoch: 12] loss: 0.19056\n",
            "[epoch: 13] loss: 0.20588\n",
            "[epoch: 14] loss: 0.22114\n",
            "[epoch: 15] loss: 0.23611\n",
            "[epoch: 16] loss: 0.25085\n",
            "[epoch: 17] loss: 0.26544\n",
            "[epoch: 18] loss: 0.27969\n",
            "[epoch: 19] loss: 0.29376\n",
            "[epoch: 20] loss: 0.30741\n",
            "[epoch: 21] loss: 0.32090\n",
            "[epoch: 22] loss: 0.33398\n",
            "[epoch: 23] loss: 0.34683\n",
            "[epoch: 24] loss: 0.35940\n",
            "[epoch: 25] loss: 0.37028\n",
            "[epoch: 26] loss: 0.38211\n",
            "[epoch: 27] loss: 0.39415\n",
            "[epoch: 28] loss: 0.40584\n",
            "[epoch: 29] loss: 0.41761\n",
            "[epoch: 30] loss: 0.42880\n",
            "[epoch: 31] loss: 0.44027\n",
            "[epoch: 32] loss: 0.45126\n",
            "[epoch: 33] loss: 0.46237\n",
            "[epoch: 34] loss: 0.47286\n",
            "[epoch: 35] loss: 0.48338\n",
            "[epoch: 36] loss: 0.49380\n",
            "[epoch: 37] loss: 0.50424\n",
            "[epoch: 38] loss: 0.51453\n",
            "[epoch: 39] loss: 0.52426\n",
            "[epoch: 40] loss: 0.53436\n",
            "[epoch: 41] loss: 0.54438\n",
            "[epoch: 42] loss: 0.55399\n",
            "[epoch: 43] loss: 0.56315\n",
            "[epoch: 44] loss: 0.57283\n",
            "[epoch: 45] loss: 0.58242\n",
            "[epoch: 46] loss: 0.59142\n",
            "[epoch: 47] loss: 0.60094\n",
            "[epoch: 48] loss: 0.60998\n",
            "[epoch: 49] loss: 0.61917\n",
            "[epoch: 50] loss: 0.62378\n",
            "[epoch: 51] loss: 0.63216\n",
            "[epoch: 52] loss: 0.64116\n",
            "[epoch: 53] loss: 0.64926\n",
            "[epoch: 54] loss: 0.65803\n",
            "[epoch: 55] loss: 0.66668\n",
            "[epoch: 56] loss: 0.67524\n",
            "[epoch: 57] loss: 0.68353\n",
            "[epoch: 58] loss: 0.69182\n",
            "[epoch: 59] loss: 0.69963\n",
            "[epoch: 60] loss: 0.70803\n",
            "[epoch: 61] loss: 0.71633\n",
            "[epoch: 62] loss: 0.72440\n",
            "[epoch: 63] loss: 0.73193\n",
            "[epoch: 64] loss: 0.73998\n",
            "[epoch: 65] loss: 0.74746\n",
            "[epoch: 66] loss: 0.75545\n",
            "[epoch: 67] loss: 0.76326\n",
            "[epoch: 68] loss: 0.77028\n",
            "[epoch: 69] loss: 0.77829\n",
            "[epoch: 70] loss: 0.78478\n",
            "[epoch: 71] loss: 0.79216\n",
            "[epoch: 72] loss: 0.79984\n",
            "[epoch: 73] loss: 0.80745\n",
            "[epoch: 74] loss: 0.81391\n",
            "[epoch: 75] loss: 0.81547\n",
            "[epoch: 76] loss: 0.82116\n",
            "[epoch: 77] loss: 0.82767\n",
            "[epoch: 78] loss: 0.83443\n",
            "[epoch: 79] loss: 0.84117\n",
            "[epoch: 80] loss: 0.84804\n",
            "[epoch: 81] loss: 0.85520\n",
            "[epoch: 82] loss: 0.86124\n",
            "[epoch: 83] loss: 0.86818\n",
            "[epoch: 84] loss: 0.87446\n",
            "[epoch: 85] loss: 0.88090\n",
            "[epoch: 86] loss: 0.88816\n",
            "[epoch: 87] loss: 0.89402\n",
            "[epoch: 88] loss: 0.90034\n",
            "[epoch: 89] loss: 0.90719\n",
            "[epoch: 90] loss: 0.91269\n",
            "[epoch: 91] loss: 0.91950\n",
            "[epoch: 92] loss: 0.92470\n",
            "[epoch: 93] loss: 0.93129\n",
            "[epoch: 94] loss: 0.93725\n",
            "[epoch: 95] loss: 0.94287\n",
            "[epoch: 96] loss: 0.94913\n",
            "[epoch: 97] loss: 0.95499\n",
            "[epoch: 98] loss: 0.96131\n",
            "[epoch: 99] loss: 0.96659\n",
            "[epoch: 100] loss: 0.96485\n",
            "[epoch: 101] loss: 0.96896\n",
            "[epoch: 102] loss: 0.97332\n",
            "[epoch: 103] loss: 0.97826\n",
            "[epoch: 104] loss: 0.98345\n",
            "[epoch: 105] loss: 0.98844\n",
            "[epoch: 106] loss: 0.99386\n",
            "[epoch: 107] loss: 0.99885\n",
            "[epoch: 108] loss: 1.00406\n",
            "[epoch: 109] loss: 1.00987\n",
            "[epoch: 110] loss: 1.01539\n",
            "[epoch: 111] loss: 1.01984\n",
            "[epoch: 112] loss: 1.02488\n",
            "[epoch: 113] loss: 1.02996\n",
            "[epoch: 114] loss: 1.03508\n",
            "[epoch: 115] loss: 1.04158\n",
            "[epoch: 116] loss: 1.04430\n",
            "[epoch: 117] loss: 1.05030\n",
            "[epoch: 118] loss: 1.05479\n",
            "[epoch: 119] loss: 1.05924\n",
            "[epoch: 120] loss: 1.06429\n",
            "[epoch: 121] loss: 1.07064\n",
            "[epoch: 122] loss: 1.07505\n",
            "[epoch: 123] loss: 1.07784\n",
            "[epoch: 124] loss: 1.08215\n",
            "[epoch: 125] loss: 1.07534\n",
            "[epoch: 126] loss: 1.07679\n",
            "[epoch: 127] loss: 1.07931\n",
            "[epoch: 128] loss: 1.08371\n",
            "[epoch: 129] loss: 1.08745\n",
            "[epoch: 130] loss: 1.09059\n",
            "[epoch: 131] loss: 1.09653\n",
            "[epoch: 132] loss: 1.10132\n",
            "[epoch: 133] loss: 1.10194\n",
            "[epoch: 134] loss: 1.10712\n",
            "[epoch: 135] loss: 1.11109\n",
            "[epoch: 136] loss: 1.11564\n",
            "[epoch: 137] loss: 1.11831\n",
            "[epoch: 138] loss: 1.12557\n",
            "[epoch: 139] loss: 1.12589\n",
            "[epoch: 140] loss: 1.13178\n",
            "[epoch: 141] loss: 1.13525\n",
            "[epoch: 142] loss: 1.13587\n",
            "[epoch: 143] loss: 1.14090\n",
            "[epoch: 144] loss: 1.14304\n",
            "[epoch: 145] loss: 1.14810\n",
            "[epoch: 146] loss: 1.15265\n",
            "[epoch: 147] loss: 1.15556\n",
            "[epoch: 148] loss: 1.15800\n",
            "[epoch: 149] loss: 1.16020\n",
            "[epoch: 150] loss: 1.14237\n",
            "[epoch: 151] loss: 1.14142\n",
            "[epoch: 152] loss: 1.14141\n",
            "[epoch: 153] loss: 1.14333\n",
            "[epoch: 154] loss: 1.14346\n",
            "[epoch: 155] loss: 1.14377\n",
            "[epoch: 156] loss: 1.14774\n",
            "[epoch: 157] loss: 1.14911\n",
            "[epoch: 158] loss: 1.15296\n",
            "[epoch: 159] loss: 1.15250\n",
            "[epoch: 160] loss: 1.15765\n",
            "[epoch: 161] loss: 1.15979\n",
            "[epoch: 162] loss: 1.15812\n",
            "[epoch: 163] loss: 1.16260\n",
            "[epoch: 164] loss: 1.16322\n",
            "[epoch: 165] loss: 1.16724\n",
            "[epoch: 166] loss: 1.16522\n",
            "[epoch: 167] loss: 1.17164\n",
            "[epoch: 168] loss: 1.17073\n",
            "[epoch: 169] loss: 1.17519\n",
            "[epoch: 170] loss: 1.17592\n",
            "[epoch: 171] loss: 1.17530\n",
            "[epoch: 172] loss: 1.18101\n",
            "[epoch: 173] loss: 1.18177\n",
            "[epoch: 174] loss: 1.17855\n",
            "[epoch: 175] loss: 1.16383\n",
            "[epoch: 176] loss: 1.15163\n",
            "[epoch: 177] loss: 1.14936\n",
            "[epoch: 178] loss: 1.14595\n",
            "[epoch: 179] loss: 1.14817\n",
            "[epoch: 180] loss: 1.14912\n",
            "[epoch: 181] loss: 1.14488\n",
            "[epoch: 182] loss: 1.14497\n",
            "[epoch: 183] loss: 1.14760\n",
            "[epoch: 184] loss: 1.14426\n",
            "[epoch: 185] loss: 1.14819\n",
            "[epoch: 186] loss: 1.14861\n",
            "[epoch: 187] loss: 1.14686\n",
            "[epoch: 188] loss: 1.14876\n",
            "[epoch: 189] loss: 1.14611\n",
            "[epoch: 190] loss: 1.14587\n",
            "[epoch: 191] loss: 1.14536\n",
            "[epoch: 192] loss: 1.15093\n",
            "[epoch: 193] loss: 1.14997\n",
            "[epoch: 194] loss: 1.14688\n",
            "[epoch: 195] loss: 1.14805\n",
            "[epoch: 196] loss: 1.14959\n",
            "[epoch: 197] loss: 1.14811\n",
            "[epoch: 198] loss: 1.15348\n",
            "[epoch: 199] loss: 1.15330\n",
            "[epoch: 200] loss: 1.13127\n",
            "[epoch: 201] loss: 1.11799\n",
            "[epoch: 202] loss: 1.11964\n",
            "[epoch: 203] loss: 1.11318\n",
            "[epoch: 204] loss: 1.11463\n",
            "[epoch: 205] loss: 1.11277\n",
            "[epoch: 206] loss: 1.10847\n",
            "[epoch: 207] loss: 1.10752\n",
            "[epoch: 208] loss: 1.10575\n",
            "[epoch: 209] loss: 1.10745\n",
            "[epoch: 210] loss: 1.10849\n",
            "[epoch: 211] loss: 1.10660\n",
            "[epoch: 212] loss: 1.10217\n",
            "[epoch: 213] loss: 1.10282\n",
            "[epoch: 214] loss: 1.09997\n",
            "[epoch: 215] loss: 1.09988\n",
            "[epoch: 216] loss: 1.10462\n",
            "[epoch: 217] loss: 1.09946\n",
            "[epoch: 218] loss: 1.10128\n",
            "[epoch: 219] loss: 1.09858\n",
            "[epoch: 220] loss: 1.10197\n",
            "[epoch: 221] loss: 1.09749\n",
            "[epoch: 222] loss: 1.09604\n",
            "[epoch: 223] loss: 1.09353\n",
            "[epoch: 224] loss: 1.09687\n",
            "[epoch: 225] loss: 1.08170\n",
            "[epoch: 226] loss: 1.07547\n",
            "[epoch: 227] loss: 1.07340\n",
            "[epoch: 228] loss: 1.06820\n",
            "[epoch: 229] loss: 1.06556\n",
            "[epoch: 230] loss: 1.06858\n",
            "[epoch: 231] loss: 1.06353\n",
            "[epoch: 232] loss: 1.05804\n",
            "[epoch: 233] loss: 1.06767\n",
            "[epoch: 234] loss: 1.06111\n",
            "[epoch: 235] loss: 1.05843\n",
            "[epoch: 236] loss: 1.05515\n",
            "[epoch: 237] loss: 1.06019\n",
            "[epoch: 238] loss: 1.05543\n",
            "[epoch: 239] loss: 1.05345\n",
            "[epoch: 240] loss: 1.05774\n",
            "[epoch: 241] loss: 1.05073\n",
            "[epoch: 242] loss: 1.05222\n",
            "[epoch: 243] loss: 1.05139\n",
            "[epoch: 244] loss: 1.05242\n",
            "[epoch: 245] loss: 1.04725\n",
            "[epoch: 246] loss: 1.04831\n",
            "[epoch: 247] loss: 1.04731\n",
            "[epoch: 248] loss: 1.04609\n",
            "[epoch: 249] loss: 1.04793\n",
            "Done!\n",
            "number of abstained set: 24877, number of kept data set: 25123\n",
            "percentage noise filtered: 0.86695\n",
            "percentage noise remained: 0.10591887911475541\n",
            "[epoch: 0] loss: 3.05468\n",
            "[epoch: 1] loss: 2.14886\n",
            "[epoch: 2] loss: 1.96323\n",
            "[epoch: 3] loss: 1.87759\n",
            "[epoch: 4] loss: 1.85391\n",
            "[epoch: 5] loss: 1.81431\n",
            "[epoch: 6] loss: 1.78619\n",
            "[epoch: 7] loss: 1.68053\n",
            "[epoch: 8] loss: 1.60223\n",
            "[epoch: 9] loss: 1.52383\n",
            "[epoch: 10] loss: 1.46663\n",
            "[epoch: 11] loss: 1.40804\n",
            "[epoch: 12] loss: 1.36782\n",
            "[epoch: 13] loss: 1.32074\n",
            "[epoch: 14] loss: 1.28859\n",
            "[epoch: 15] loss: 1.24446\n",
            "[epoch: 16] loss: 1.22386\n",
            "[epoch: 17] loss: 1.18652\n",
            "[epoch: 18] loss: 1.16463\n",
            "[epoch: 19] loss: 1.13364\n",
            "[epoch: 20] loss: 1.13369\n",
            "[epoch: 21] loss: 1.12505\n",
            "[epoch: 22] loss: 1.11196\n",
            "[epoch: 23] loss: 1.09656\n",
            "[epoch: 24] loss: 1.08474\n",
            "[epoch: 25] loss: 0.95911\n",
            "[epoch: 26] loss: 0.93996\n",
            "[epoch: 27] loss: 0.92092\n",
            "[epoch: 28] loss: 0.92239\n",
            "[epoch: 29] loss: 0.91297\n",
            "[epoch: 30] loss: 0.91227\n",
            "[epoch: 31] loss: 0.90520\n",
            "[epoch: 32] loss: 0.89463\n",
            "[epoch: 33] loss: 0.89698\n",
            "[epoch: 34] loss: 0.88972\n",
            "[epoch: 35] loss: 0.88788\n",
            "[epoch: 36] loss: 0.87821\n",
            "[epoch: 37] loss: 0.86906\n",
            "[epoch: 38] loss: 0.86240\n",
            "[epoch: 39] loss: 0.86593\n",
            "[epoch: 40] loss: 0.86456\n",
            "[epoch: 41] loss: 0.86268\n",
            "[epoch: 42] loss: 0.85594\n",
            "[epoch: 43] loss: 0.85339\n",
            "[epoch: 44] loss: 0.84415\n",
            "[epoch: 45] loss: 0.84978\n",
            "[epoch: 46] loss: 0.83174\n",
            "[epoch: 47] loss: 0.83550\n",
            "[epoch: 48] loss: 0.82747\n",
            "[epoch: 49] loss: 0.82259\n",
            "[epoch: 50] loss: 0.73000\n",
            "[epoch: 51] loss: 0.70593\n",
            "[epoch: 52] loss: 0.68937\n",
            "[epoch: 53] loss: 0.68431\n",
            "[epoch: 54] loss: 0.69014\n",
            "[epoch: 55] loss: 0.68167\n",
            "[epoch: 56] loss: 0.67789\n",
            "[epoch: 57] loss: 0.66063\n",
            "[epoch: 58] loss: 0.67677\n",
            "[epoch: 59] loss: 0.67453\n",
            "[epoch: 60] loss: 0.66155\n",
            "[epoch: 61] loss: 0.66979\n",
            "[epoch: 62] loss: 0.66417\n",
            "[epoch: 63] loss: 0.66521\n",
            "[epoch: 64] loss: 0.65336\n",
            "[epoch: 65] loss: 0.64773\n",
            "[epoch: 66] loss: 0.64873\n",
            "[epoch: 67] loss: 0.64843\n",
            "[epoch: 68] loss: 0.64545\n",
            "[epoch: 69] loss: 0.63985\n",
            "[epoch: 70] loss: 0.64363\n",
            "[epoch: 71] loss: 0.64776\n",
            "[epoch: 72] loss: 0.63499\n",
            "[epoch: 73] loss: 0.63113\n",
            "[epoch: 74] loss: 0.63590\n",
            "[epoch: 75] loss: 0.55280\n",
            "[epoch: 76] loss: 0.52421\n",
            "[epoch: 77] loss: 0.51141\n",
            "[epoch: 78] loss: 0.50401\n",
            "[epoch: 79] loss: 0.49382\n",
            "[epoch: 80] loss: 0.50012\n",
            "[epoch: 81] loss: 0.49036\n",
            "[epoch: 82] loss: 0.48084\n",
            "[epoch: 83] loss: 0.47916\n",
            "[epoch: 84] loss: 0.48455\n",
            "[epoch: 85] loss: 0.47807\n",
            "[epoch: 86] loss: 0.47455\n",
            "[epoch: 87] loss: 0.46695\n",
            "[epoch: 88] loss: 0.46925\n",
            "[epoch: 89] loss: 0.47045\n",
            "[epoch: 90] loss: 0.45654\n",
            "[epoch: 91] loss: 0.45165\n",
            "[epoch: 92] loss: 0.46280\n",
            "[epoch: 93] loss: 0.45095\n",
            "[epoch: 94] loss: 0.45469\n",
            "[epoch: 95] loss: 0.44509\n",
            "[epoch: 96] loss: 0.45239\n",
            "[epoch: 97] loss: 0.45062\n",
            "[epoch: 98] loss: 0.43810\n",
            "[epoch: 99] loss: 0.44253\n",
            "[epoch: 100] loss: 0.37104\n",
            "[epoch: 101] loss: 0.35096\n",
            "[epoch: 102] loss: 0.33814\n",
            "[epoch: 103] loss: 0.33470\n",
            "[epoch: 104] loss: 0.31667\n",
            "[epoch: 105] loss: 0.32209\n",
            "[epoch: 106] loss: 0.32180\n",
            "[epoch: 107] loss: 0.30547\n",
            "[epoch: 108] loss: 0.31934\n",
            "[epoch: 109] loss: 0.30787\n",
            "[epoch: 110] loss: 0.29931\n",
            "[epoch: 111] loss: 0.29497\n",
            "[epoch: 112] loss: 0.29390\n",
            "[epoch: 113] loss: 0.28941\n",
            "[epoch: 114] loss: 0.29830\n",
            "[epoch: 115] loss: 0.28952\n",
            "[epoch: 116] loss: 0.29431\n",
            "[epoch: 117] loss: 0.28176\n",
            "[epoch: 118] loss: 0.28833\n",
            "[epoch: 119] loss: 0.27341\n",
            "[epoch: 120] loss: 0.27901\n",
            "[epoch: 121] loss: 0.28103\n",
            "[epoch: 122] loss: 0.27748\n",
            "[epoch: 123] loss: 0.26647\n",
            "[epoch: 124] loss: 0.28030\n",
            "[epoch: 125] loss: 0.23088\n",
            "[epoch: 126] loss: 0.21459\n",
            "[epoch: 127] loss: 0.20291\n",
            "[epoch: 128] loss: 0.19823\n",
            "[epoch: 129] loss: 0.19649\n",
            "[epoch: 130] loss: 0.19768\n",
            "[epoch: 131] loss: 0.18823\n",
            "[epoch: 132] loss: 0.18508\n",
            "[epoch: 133] loss: 0.18508\n",
            "[epoch: 134] loss: 0.18388\n",
            "[epoch: 135] loss: 0.17962\n",
            "[epoch: 136] loss: 0.18219\n",
            "[epoch: 137] loss: 0.17492\n",
            "[epoch: 138] loss: 0.17079\n",
            "[epoch: 139] loss: 0.17159\n",
            "[epoch: 140] loss: 0.16858\n",
            "[epoch: 141] loss: 0.17147\n",
            "[epoch: 142] loss: 0.16753\n",
            "[epoch: 143] loss: 0.17106\n",
            "[epoch: 144] loss: 0.17269\n",
            "[epoch: 145] loss: 0.16624\n",
            "[epoch: 146] loss: 0.16804\n",
            "[epoch: 147] loss: 0.16111\n",
            "[epoch: 148] loss: 0.17157\n",
            "[epoch: 149] loss: 0.16471\n",
            "[epoch: 150] loss: 0.14522\n",
            "[epoch: 151] loss: 0.12826\n",
            "[epoch: 152] loss: 0.13330\n",
            "[epoch: 153] loss: 0.12951\n",
            "[epoch: 154] loss: 0.12031\n",
            "[epoch: 155] loss: 0.12727\n",
            "[epoch: 156] loss: 0.12097\n",
            "[epoch: 157] loss: 0.12042\n",
            "[epoch: 158] loss: 0.11948\n",
            "[epoch: 159] loss: 0.11546\n",
            "[epoch: 160] loss: 0.10794\n",
            "[epoch: 161] loss: 0.11935\n",
            "[epoch: 162] loss: 0.10471\n",
            "[epoch: 163] loss: 0.10772\n",
            "[epoch: 164] loss: 0.11494\n",
            "[epoch: 165] loss: 0.11343\n",
            "[epoch: 166] loss: 0.10915\n",
            "[epoch: 167] loss: 0.10583\n",
            "[epoch: 168] loss: 0.11098\n",
            "[epoch: 169] loss: 0.10673\n",
            "[epoch: 170] loss: 0.10638\n",
            "[epoch: 171] loss: 0.11044\n",
            "[epoch: 172] loss: 0.10281\n",
            "[epoch: 173] loss: 0.10855\n",
            "[epoch: 174] loss: 0.10444\n",
            "[epoch: 175] loss: 0.10032\n",
            "[epoch: 176] loss: 0.09310\n",
            "[epoch: 177] loss: 0.08970\n",
            "[epoch: 178] loss: 0.09125\n",
            "[epoch: 179] loss: 0.09379\n",
            "[epoch: 180] loss: 0.08626\n",
            "[epoch: 181] loss: 0.08839\n",
            "[epoch: 182] loss: 0.08537\n",
            "[epoch: 183] loss: 0.08459\n",
            "[epoch: 184] loss: 0.08963\n",
            "[epoch: 185] loss: 0.07835\n",
            "[epoch: 186] loss: 0.08197\n",
            "[epoch: 187] loss: 0.08291\n",
            "[epoch: 188] loss: 0.07954\n",
            "[epoch: 189] loss: 0.08319\n",
            "[epoch: 190] loss: 0.07708\n",
            "[epoch: 191] loss: 0.07691\n",
            "[epoch: 192] loss: 0.07713\n",
            "[epoch: 193] loss: 0.07991\n",
            "[epoch: 194] loss: 0.07880\n",
            "[epoch: 195] loss: 0.07867\n",
            "[epoch: 196] loss: 0.08215\n",
            "[epoch: 197] loss: 0.07874\n",
            "[epoch: 198] loss: 0.07716\n",
            "[epoch: 199] loss: 0.07843\n",
            "[epoch: 200] loss: 0.07538\n",
            "[epoch: 201] loss: 0.07251\n",
            "[epoch: 202] loss: 0.07347\n",
            "[epoch: 203] loss: 0.06884\n",
            "[epoch: 204] loss: 0.07547\n",
            "[epoch: 205] loss: 0.07050\n",
            "[epoch: 206] loss: 0.07028\n",
            "[epoch: 207] loss: 0.06718\n",
            "[epoch: 208] loss: 0.07026\n",
            "[epoch: 209] loss: 0.07168\n",
            "[epoch: 210] loss: 0.06858\n",
            "[epoch: 211] loss: 0.07256\n",
            "[epoch: 212] loss: 0.06454\n",
            "[epoch: 213] loss: 0.06761\n",
            "[epoch: 214] loss: 0.06850\n",
            "[epoch: 215] loss: 0.06908\n",
            "[epoch: 216] loss: 0.06324\n",
            "[epoch: 217] loss: 0.06786\n",
            "[epoch: 218] loss: 0.06680\n",
            "[epoch: 219] loss: 0.06267\n",
            "[epoch: 220] loss: 0.06555\n",
            "[epoch: 221] loss: 0.06780\n",
            "[epoch: 222] loss: 0.06463\n",
            "[epoch: 223] loss: 0.05908\n",
            "[epoch: 224] loss: 0.06547\n",
            "[epoch: 225] loss: 0.05968\n",
            "[epoch: 226] loss: 0.06131\n",
            "[epoch: 227] loss: 0.06504\n",
            "[epoch: 228] loss: 0.06293\n",
            "[epoch: 229] loss: 0.06018\n",
            "[epoch: 230] loss: 0.06106\n",
            "[epoch: 231] loss: 0.06469\n",
            "[epoch: 232] loss: 0.05870\n",
            "[epoch: 233] loss: 0.06222\n",
            "[epoch: 234] loss: 0.05803\n",
            "[epoch: 235] loss: 0.06113\n",
            "[epoch: 236] loss: 0.05681\n",
            "[epoch: 237] loss: 0.06143\n",
            "[epoch: 238] loss: 0.05753\n",
            "[epoch: 239] loss: 0.05968\n",
            "[epoch: 240] loss: 0.06125\n",
            "[epoch: 241] loss: 0.06120\n",
            "[epoch: 242] loss: 0.06310\n",
            "[epoch: 243] loss: 0.06040\n",
            "[epoch: 244] loss: 0.06277\n",
            "[epoch: 245] loss: 0.05753\n",
            "[epoch: 246] loss: 0.06033\n",
            "[epoch: 247] loss: 0.05997\n",
            "[epoch: 248] loss: 0.05598\n",
            "[epoch: 249] loss: 0.06047\n",
            "Accuracy of the network on the 10000 test images: 77.06 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for noise_level in [0, 0.1, 0.2, 0.4]: \n",
        "    print(\"Test post_dac at noise level {}\".format(noise_level))\n",
        "\n",
        "    dataloader = cifar10_data_loader(batch_size, shuffle=False)\n",
        "    trainloader, testloader, trainset, correct_target, noise_indexes = dataloader.load(noise_level=noise_level)\n",
        "    new_model_name = 'vgg16_post_dac_dnn_nr_{}_epoch_{}'.format(noise_level, epoches)\n",
        "    new_model = VGG('VGG16_upd', False).to(device)\n",
        "    new_model.load_state_dict(torch.load(dir_name + new_model_name +'.pth'))\n",
        "    new_model.eval()\n",
        "\n",
        "\n",
        "    kappa, residuals = evaluate(new_model, testloader)\n",
        "    # plot_cr(kappa, residuals, dir_name + 'cr_vgg16_post_dac_dnn_nr_{}_epoch_{}.png'.format(noise_level, epoches))\n",
        "    print()\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aDDHhFUvieY",
        "outputId": "fa67fd9e-9f3d-45a2-80f5-51010dcc8b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test post_dac at noise level 0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "tensor([0.9300, 0.9690, 0.9150, 0.8570, 0.9550, 0.8710, 0.9520, 0.9500, 0.9680,\n",
            "        0.9560])\n",
            "Accuracy of the network on the 10000 test images: 93.23 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Test post_dac at noise level 0.1\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "tensor([0.8690, 0.9080, 0.8180, 0.7720, 0.8930, 0.7920, 0.9100, 0.9080, 0.9460,\n",
            "        0.9070])\n",
            "Accuracy of the network on the 10000 test images: 87.23 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Test post_dac at noise level 0.2\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "tensor([0.8920, 0.9670, 0.8700, 0.7820, 0.8710, 0.8470, 0.9360, 0.9040, 0.9400,\n",
            "        0.9030])\n",
            "Accuracy of the network on the 10000 test images: 89.12 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n",
            "Test post_dac at noise level 0.4\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "tensor([0.7890, 0.9160, 0.7580, 0.6310, 0.8230, 0.7680, 0.8990, 0.8630, 0.9190,\n",
            "        0.8280])\n",
            "Accuracy of the network on the 10000 test images: 81.94 %\n",
            "\n",
            "------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}